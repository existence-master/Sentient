### agents/startserv.sh ###
sudo -E <your-venv-path> -m uvicorn agents:app --host 0.0.0.0 --port 5001
### agents/agents.py ###
import os
import uvicorn
import json
import asyncio  # Import asyncio for asynchronous operations
from runnables import *
from functions import *
from externals import *
from helpers import *
from prompts import *
import nest_asyncio
from fastapi import FastAPI
from pydantic import BaseModel
from typing import (
    Optional,
    Any,
    Dict,
    List,
    AsyncGenerator,
)  # Import specific types for clarity
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from datetime import datetime
from tzlocal import get_localzone
from dotenv import load_dotenv

load_dotenv("../.env")  # Load environment variables from .env file

# --- FastAPI Application ---
app = FastAPI(
    title="Sentient API", description="API for the Sentient AI companion", docs_url="/docs", redoc_url=None
)  # Initialize FastAPI application

# --- CORS Middleware ---
# Configure CORS to allow cross-origin requests.
# In a production environment, you should restrict the `allow_origins` to specific domains for security.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins - configure this for production
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods - configure this for production
    allow_headers=["*"],  # Allows all headers - configure this for production
)

# --- Tool Handlers Registry ---
# Dictionary to store registered tool handler functions.
tool_handlers: Dict[str, callable] = {}


def register_tool(name: str):
    """
    Decorator to register a function as a tool handler.

    Args:
        name (str): The name of the tool to register.

    Returns:
        callable: A decorator that registers the decorated function as a tool handler.
    """

    def decorator(func: callable):
        """
        The actual decorator function that registers the tool.

        Args:
            func (callable): The function to be registered as a tool handler.

        Returns:
            callable: The original function, after registering it in tool_handlers.
        """
        tool_handlers[name] = func
        return func

    return decorator


# --- Apply nest_asyncio ---
# nest_asyncio is used to allow asyncio.run() to be called from within a jupyter notebook or another async environment.
# It's needed here because uvicorn runs in an asyncio event loop, and we might need to run async functions within the API endpoints.
nest_asyncio.apply()

# --- Pydantic Models for Request Bodies ---


class Message(BaseModel):
    """
    Pydantic model for the chat message request body.

    Attributes:
        original_input (str): The original user input message.
        transformed_input (str): The transformed user input message, potentially after preprocessing.
        pricing (str): The pricing plan of the user (e.g., "free", "pro").
        credits (int): The number of credits the user has.
        chat_id (str): Unique identifier for the chat session.
    """

    original_input: str
    transformed_input: str
    pricing: str
    credits: int
    chat_id: str


class ToolCall(BaseModel):
    """
    Pydantic model for tool call requests.

    Attributes:
        input (str): The input string for the tool.
        previous_tool_response (Optional[Any]): The response from a previous tool call, if required. Defaults to None.
    """

    input: str
    previous_tool_response: Optional[Any] = None


class ElaboratorMessage(BaseModel):
    """
    Pydantic model for the elaborator message request body.

    Attributes:
        input (str): The input string to be elaborated.
        purpose (str): The purpose of elaboration.
    """

    input: str
    purpose: str


# --- Global Variables ---
# These global variables hold the runnables for different parts of the application.
# It is initialized to None and will be set in the `/initiate` endpoint.
chat_history = None  # Placeholder for chat history object
chat_runnable = None  # Runnable for handling chat conversations
agent_runnable = None  # Runnable for the main agent logic
tool_runnable = None  # Runnable for handling tool calls
reflection_runnable = None  # Runnable for reflection process
inbox_summarizer_runnable = None  # Runnable for summarizing inbox contents

# --- API Endpoints ---


@app.get("/", status_code=200)
async def main() -> Dict[str, str]:
    """
    Root endpoint of the API.

    Returns:
        JSONResponse: A simple greeting message.
    """
    return {
        "message": "Hello, I am Sentient, your private, decentralized and interactive AI companion who feels human"
    }


@app.post("/initiate", status_code=200)
async def initiate() -> JSONResponse:
    """
    Endpoint to initiate the AI model and related runnables.

    This endpoint initializes the reflection and inbox summarizer runnables.
    It is intended to be called once at the start of a session or application lifecycle.

    Returns:
        JSONResponse: Success or error message in JSON format.
    """
    global chat_history, tool_runnable, reflection_runnable, inbox_summarizer_runnable

    reflection_runnable = get_reflection_runnable()  # Initialize reflection runnable
    inbox_summarizer_runnable = (
        get_inbox_summarizer_runnable()
    )  # Initialize inbox summarizer runnable

    try:
        return JSONResponse(
            status_code=200, content={"message": "Model initiated successfully"}
        )
    except Exception as e:
        print(f"Error in initiating agents: {str(e)}")
        return JSONResponse(status_code=500, content={"message": str(e)})


@app.post("/elaborator", status_code=200)
async def elaborate(message: ElaboratorMessage) -> JSONResponse:
    try:
        elaborator_runnable = get_tool_runnable(
            elaborator_system_prompt_template,
            elaborator_user_prompt_template,
            None,
            ["query", "purpose"],
        )
        output = elaborator_runnable.invoke(
            {"query": message.input, "purpose": message.purpose}
        )
        print(f"Elaborator output: {output}")  # Debug the raw output
        return JSONResponse(status_code=200, content={"message": output})
    except Exception as e:
        print(f"Error in elaborator: {str(e)}")
        return JSONResponse(status_code=500, content={"message": str(e)})


@app.post("/chat", status_code=200)
async def chat(message: Message) -> StreamingResponse:
    """
    Endpoint to handle chat messages and generate responses using the AI model.

    This is the main chat endpoint that processes user messages, retrieves context,
    calls tools if necessary, and streams back the response.

    Args:
        message (Message): Request body containing the chat message details.

    Returns:
        StreamingResponse: A streaming response containing different types of messages
                           (user message, intermediary messages, assistant messages, tool results).
    """
    global chat_runnable, agent_runnable, reflection_runnable

    try:
        with open(
            "../../userProfileDb.json", "r", encoding="utf-8"
        ) as f:  # Load user profile database
            db = json.load(f)

        chat_history = get_chat_history()  # Retrieve chat history for the given chat ID

        chat_runnable = get_chat_runnable(
            chat_history
        )  # Initialize chat runnable with chat history
        agent_runnable = get_agent_runnable(
            chat_history
        )  # Initialize agent runnable with chat history
        username = db["userData"]["personalInfo"][
            "name"
        ]  # Extract username from user profile

        transformed_input = (
            message.transformed_input
        )  # Get transformed input from message
        pricing_plan = message.pricing  # Get pricing plan from message
        credits = message.credits  # Get credits from message

        async def response_generator() -> AsyncGenerator[str, None]:
            """
            Asynchronous generator to produce streaming responses for the chat endpoint.

            Yields:
                str: JSON string representing different types of messages in the chat flow.
            """
            memory_used = False  # Flag to track if memory was used
            agents_used = False  # Flag to track if agents were used
            internet_used = False  # Flag to track if internet search was used
            user_context = None  # Placeholder for user context retrieved from memory
            internet_context = None  # Placeholder for internet search results
            pro_used = False  # Flag to track if pro features were used
            note = ""  # Placeholder for notes or messages to the user

            agents_used = True  # Mark agents as used for this interaction

            yield (
                json.dumps(
                    {
                        "type": "userMessage",
                        "message": message.original_input,
                        "memoryUsed": False,
                        "agentsUsed": False,
                        "internetUsed": False,
                    }
                )
                + "\n"
            )  # Yield user message
            await asyncio.sleep(0.05)  # Small delay for streaming effect

            context_classification = await classify_context(
                transformed_input, "category"
            )  # Classify context for memory retrieval

            if (
                "personal" in context_classification
            ):  # If context is personal, retrieve memory
                yield (
                    json.dumps(
                        {"type": "intermediary", "message": "Retrieving memories..."}
                    )
                    + "\n"
                )  # Yield intermediary message
                await asyncio.sleep(0.05)  # Small delay
                memory_used = True  # Mark memory as used
                user_context = await perform_graphrag(
                    transformed_input
                )  # Perform graph-based RAG for memory retrieval
            else:
                user_context = None  # No user context needed

            internet_classification = await classify_context(
                transformed_input, "internet"
            )  # Classify context for internet search

            if pricing_plan == "free":  # Free plan logic
                if (
                    internet_classification == "Internet"
                ):  # If internet search is relevant
                    if credits > 0:  # Check for credits
                        yield (
                            json.dumps(
                                {
                                    "type": "intermediary",
                                    "message": "Searching the internet...",
                                }
                            )
                            + "\n"
                        )  # Yield intermediary message
                        internet_context = await perform_internet_search(
                            transformed_input
                        )  # Perform internet search
                        internet_used = True  # Mark internet as used
                        pro_used = True  # Mark pro feature as used
                    else:
                        note = "Could have searched the internet too. But, that is a pro feature too :)"  # Note for free users without credits
                else:
                    internet_context = None  # No internet context needed for free plan
            else:  # Pro plan logic
                internet_classification = await classify_context(
                    transformed_input, "category"
                )  # Classify context for internet search

                if (
                    internet_classification == "Internet"
                ):  # If internet search is relevant
                    yield (
                        json.dumps(
                            {
                                "type": "intermediary",
                                "message": "Searching the internet...",
                            }
                        )
                        + "\n"
                    )  # Yield intermediary message
                    internet_context = await perform_internet_search(
                        transformed_input
                    )  # Perform internet search
                    internet_used = True  # Mark internet as used
                    pro_used = True  # Mark pro feature as used
                else:
                    internet_context = None  # No internet context needed for pro plan

            response = generate_response(
                agent_runnable,
                transformed_input,
                user_context,
                internet_context,
                username,
            )  # Generate agent response

            if "tool_calls" not in response or not isinstance(
                response["tool_calls"], list
            ):  # Check for valid tool calls in response
                yield (
                    json.dumps(
                        {
                            "type": "assistantMessage",
                            "message": "Error: Invalid tool_calls format in response.",  # Error message for invalid tool calls
                        }
                    )
                    + "\n"
                )
                return

            previous_tool_result = None  # Placeholder for previous tool result
            all_tool_results: List[
                Dict
            ] = []  # List to store results from all tool calls

            if (
                len(response["tool_calls"]) > 1 and pricing_plan == "free"
            ):  # Check for multiple tool calls in free plan
                if credits <= 0:  # Check for credits in free plan for multiple tools
                    yield (
                        json.dumps(
                            {
                                "type": "assistantMessage",
                                "message": "Sorry friend, but the query requires multiple tools to be called. This is a pro feature and you are out of daily credits for pro. You can upgrade to pro from the settings page."
                                + f"\n\n{note}",  # Message for free users without credits for multiple tools
                            }
                        )
                        + "\n"
                    )
                    return
                else:
                    pro_used = True  # Mark pro feature as used if credits are available

            for tool_call in response[
                "tool_calls"
            ]:  # Iterate through tool calls in the response
                if (
                    tool_call["response_type"] != "tool_call"
                ):  # Skip if not a tool call response
                    continue

                tool_name = tool_call["content"].get(
                    "tool_name"
                )  # Get tool name from tool call content

                if (
                    tool_name != "gmail" and pricing_plan == "free"
                ):  # Check for tool usage in free plan (excluding gmail)
                    if credits <= 0:  # Check for credits in free plan for tool usage
                        yield (
                            json.dumps(
                                {
                                    "type": "assistantMessage",
                                    "message": "Sorry friend but the query requires a tool to be called which is only available in the pro version. This is a pro feature and you are out of daily credits for pro. You can upgrade to pro from the settings page."
                                    + f"\n\n{note}",  # Message for free users without credits for tool usage
                                }
                            )
                            + "\n"
                        )
                        return
                    else:
                        pro_used = (
                            True  # Mark pro feature as used if credits are available
                        )

                task_instruction = tool_call["content"].get(
                    "task_instruction"
                )  # Get task instruction from tool call content
                previous_tool_response_required = tool_call["content"].get(
                    "previous_tool_response", False
                )  # Check if previous tool response is required

                if (
                    not tool_name or not task_instruction
                ):  # Check for required fields in tool call
                    yield (
                        json.dumps(
                            {
                                "type": "assistantMessage",
                                "message": "Error: Tool call is missing required fields.",  # Error message for missing tool call fields
                            }
                        )
                        + "\n"
                    )
                    continue

                yield (
                    json.dumps(
                        {
                            "type": "intermediary-flow-update",
                            "message": f"Calling tool: {tool_name}...",
                        }
                    )
                    + "\n"
                )  # Yield intermediary message - tool call update
                await asyncio.sleep(0.05)  # Small delay

                tool_handler = tool_handlers.get(
                    tool_name
                )  # Get tool handler function from registry
                if not tool_handler:  # Check if tool handler exists
                    yield (
                        json.dumps(
                            {
                                "type": "assistantMessage",
                                "message": f"Error: Tool {tool_name} not found.",  # Error message for tool not found
                            }
                        )
                        + "\n"
                    )
                    continue

                tool_input = {"input": task_instruction}  # Prepare tool input
                if (
                    previous_tool_response_required and previous_tool_result
                ):  # Add previous tool response to input if required
                    tool_input["previous_tool_response"] = previous_tool_result
                else:
                    tool_input["previous_tool_response"] = (
                        "Not Required"  # Indicate previous tool response is not required
                    )

                try:
                    tool_result_main = await tool_handler(
                        tool_input
                    )  # Execute tool handler function
                    tool_result = None
                    tool_call_str = None
                    if (
                        tool_result_main["tool_call_str"] is not None
                    ):  # Check if tool call string is present in tool result
                        tool_call_str = tool_result_main[
                            "tool_call_str"
                        ]  # Get tool call string
                        tool_result = tool_result_main["tool_result"]  # Get tool result
                        tool_name = tool_call_str[
                            "tool_name"
                        ]  # Get tool name from tool call string
                        if (
                            tool_name == "search_inbox"
                        ):  # Handle search inbox tool result
                            yield (
                                json.dumps(
                                    {
                                        "type": "toolResult",
                                        "tool_name": tool_name,
                                        "result": tool_result["result"],
                                        "gmail_search_url": tool_result["result"][
                                            "gmail_search_url"
                                        ],  # Include gmail search URL in result
                                    }
                                )
                                + "\n"
                            )
                        elif (
                            tool_name == "get_email_details"
                        ):  # Handle get email details tool result
                            yield (
                                json.dumps(
                                    {
                                        "type": "toolResult",
                                        "tool_name": tool_name,
                                        "result": tool_result["result"],
                                    }
                                )
                                + "\n"
                            )
                        await asyncio.sleep(0.05)  # Small delay
                    else:
                        tool_result = tool_result_main[
                            "tool_result"
                        ]  # Get tool result if no tool call string
                    previous_tool_result = tool_result  # Update previous tool result
                    all_tool_results.append(
                        {  # Append tool result to list
                            "tool_name": tool_name,
                            "task_instruction": task_instruction,
                            "tool_result": tool_result,
                        }
                    )

                except Exception as e:  # Handle exceptions during tool execution
                    print(f"Error executing tool {tool_name}")
                    yield (
                        json.dumps(
                            {
                                "type": "assistantMessage",
                                "message": f"Error executing tool {tool_name}: {str(e)}",  # Error message for tool execution failure
                            }
                        )
                        + "\n"
                    )
                    continue

            yield (
                json.dumps({"type": "intermediary-flow-end"}) + "\n"
            )  # Yield intermediary message - flow end
            await asyncio.sleep(0.05)  # Small delay

            try:
                if (
                    len(all_tool_results) == 1
                    and all_tool_results[0]["tool_name"] == "search_inbox"
                ):  # Handle inbox summarization for single search inbox tool result
                    filtered_tool_result = {
                        "response": all_tool_results[0]["tool_result"]["result"][
                            "response"
                        ],  # Extract response from tool result
                        "email_data": [  # Filter email data to exclude email body
                            {key: email[key] for key in email if key != "body"}
                            for email in all_tool_results[0]["tool_result"]["result"][
                                "email_data"
                            ]
                        ],
                        "gmail_search_url": all_tool_results[0]["tool_result"][
                            "result"
                        ]["gmail_search_url"],  # Extract gmail search URL
                    }

                    async for token in generate_streaming_response(  # Stream response from inbox summarizer runnable
                        inbox_summarizer_runnable,
                        inputs={"tool_result": filtered_tool_result},
                        stream=True,
                    ):
                        if isinstance(token, str):  # Yield assistant stream tokens
                            yield (
                                json.dumps(
                                    {
                                        "type": "assistantStream",
                                        "token": token,
                                        "done": False,
                                    }
                                )
                                + "\n"
                            )
                            await asyncio.sleep(0.05)
                        else:  # Yield final assistant stream message with metadata
                            yield (
                                json.dumps(
                                    {
                                        "type": "assistantStream",
                                        "token": "\n\n" + note,
                                        "done": True,
                                        "memoryUsed": memory_used,
                                        "agentsUsed": agents_used,
                                        "internetUsed": internet_used,
                                        "proUsed": pro_used,
                                    }
                                )
                                + "\n"
                            )
                        await asyncio.sleep(0.05)
                    await asyncio.sleep(0.05)
                else:  # Handle reflection for other tool results or multiple tool results
                    async for token in generate_streaming_response(  # Stream response from reflection runnable
                        reflection_runnable,
                        inputs={"tool_results": all_tool_results},
                        stream=True,
                    ):
                        if isinstance(token, str):  # Yield assistant stream tokens
                            yield (
                                json.dumps(
                                    {
                                        "type": "assistantStream",
                                        "token": token,
                                        "done": False,
                                        "memoryUsed": memory_used,
                                        "agentsUsed": agents_used,
                                        "internetUsed": internet_used,
                                        "proUsed": pro_used,
                                    }
                                )
                                + "\n"
                            )
                            await asyncio.sleep(0.05)
                        else:  # Yield final assistant stream message with metadata
                            yield (
                                json.dumps(
                                    {
                                        "type": "assistantStream",
                                        "token": "\n\n" + note,
                                        "done": True,
                                        "memoryUsed": memory_used,
                                        "agentsUsed": agents_used,
                                        "internetUsed": internet_used,
                                        "proUsed": pro_used,
                                    }
                                )
                                + "\n"
                            )
                        await asyncio.sleep(0.05)
                await asyncio.sleep(0.05)
            except (
                Exception
            ) as e:  # Handle exceptions during reflection or summarization
                print(f"Error during reflection: {e}")
                yield (
                    json.dumps(
                        {
                            "type": "assistantMessage",
                            "message": f"Error during reflection: {str(e)}",  # Error message for reflection failure
                        }
                    )
                    + "\n"
                )

        return StreamingResponse(
            response_generator(), media_type="application/json"
        )  # Return streaming response

    except Exception as e:  # Handle exceptions during chat processing
        print(f"Error during chat: {e}")
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return JSON error response


@register_tool("gmail")
async def gmail_tool(tool_call: ToolCall) -> Dict[str, Any]:
    """
    Gmail Tool endpoint to handle email related tasks using multi-tool support.
    Registered as a tool with the name "gmail".

    Args:
        tool_call (ToolCall): Request body containing the input for the gmail tool.

    Returns:
        Dict[str, Any]: A dictionary containing the tool result and tool call string.
                         Returns status "failure" and error message if an exception occurs.
    """
    try:
        with open(
            "../../userProfileDb.json", "r", encoding="utf-8"
        ) as f:  # Load user profile database
            db = json.load(f)

        username = db["userData"]["personalInfo"][
            "name"
        ]  # Extract username from user profile

        tool_runnable = get_tool_runnable(  # Initialize gmail tool runnable
            gmail_agent_system_prompt_template,
            gmail_agent_user_prompt_template,
            gmail_agent_required_format,
            [
                "query",
                "username",
                "previous_tool_response",
            ],  # Expected input parameters
        )

        tool_call_str = tool_runnable.invoke(
            {  # Invoke the gmail tool runnable
                "query": tool_call["input"],
                "username": username,
                "previous_tool_response": tool_call["previous_tool_response"],
            }
        )

        tool_result = await parse_and_execute_tool_calls(
            tool_call_str
        )  # Parse and execute tool calls from the response

        return {
            "tool_result": tool_result,
            "tool_call_str": tool_call_str,
        }  # Return tool result and tool call string
    except Exception as e:  # Handle exceptions during gmail tool execution
        print(f"Error calling gmail tool: {e}")
        return {"status": "failure", "error": str(e)}  # Return error status and message


@register_tool("gdrive")
async def drive_tool(tool_call: ToolCall) -> Dict[str, Any]:
    """
    Drive Tool endpoint to handle Google Drive interactions using multi-tool support.
    Registered as a tool with the name "gdrive".

    Args:
        tool_call (ToolCall): Request body containing the input for the drive tool.

    Returns:
        Dict[str, Any]: A dictionary containing the tool result and tool call string (None in this case).
                         Returns status "failure" and error message if an exception occurs.
    """
    try:
        tool_runnable = get_tool_runnable(  # Initialize gdrive tool runnable
            gdrive_agent_system_prompt_template,
            gdrive_agent_user_prompt_template,
            gdrive_agent_required_format,
            ["query", "previous_tool_response"],  # Expected input parameters
        )
        tool_call_str = tool_runnable.invoke(
            {  # Invoke the gdrive tool runnable
                "query": tool_call["input"],
                "previous_tool_response": tool_call["previous_tool_response"],
            }
        )

        tool_result = await parse_and_execute_tool_calls(
            tool_call_str
        )  # Parse and execute tool calls from the response
        return {
            "tool_result": tool_result,
            "tool_call_str": None,
        }  # Return tool result and None for tool call string
    except Exception as e:  # Handle exceptions during gdrive tool execution
        print(f"Error calling gdrive tool: {e}")
        return {"status": "failure", "error": str(e)}  # Return error status and message


@register_tool("gslides")
async def gslides_tool(tool_call: ToolCall) -> Dict[str, Any]:
    """
    GSlides Tool endpoint to handle Google Slides presentation creation using multi-tool support.
    Registered as a tool with the name "gslides".

    Args:
        tool_call (ToolCall): Request body containing the input for the gslides tool.

    Returns:
        Dict[str, Any]: A dictionary containing the tool result and tool call string (None in this case).
                         Returns status "failure" and error message if an exception occurs.
    """

    try:
        with open(
            "../../userProfileDb.json", "r", encoding="utf-8"
        ) as f:  # Load user profile database
            db = json.load(f)

        username = db["userData"]["personalInfo"][
            "name"
        ]  # Extract username from user profile

        tool_runnable = get_tool_runnable(  # Initialize gslides tool runnable
            gslides_agent_system_prompt_template,
            gslides_agent_user_prompt_template,
            gslides_agent_required_format,
            [
                "query",
                "user_name",
                "previous_tool_response",
            ],  # Expected input parameters
        )
        tool_call_str = tool_runnable.invoke(
            {  # Invoke the gslides tool runnable
                "query": tool_call["input"],
                "user_name": username,
                "previous_tool_response": tool_call["previous_tool_response"],
            }
        )

        tool_result = await parse_and_execute_tool_calls(
            tool_call_str
        )  # Parse and execute tool calls from the response
        return {
            "tool_result": tool_result,
            "tool_call_str": None,
        }  # Return tool result and None for tool call string
    except Exception as e:  # Handle exceptions during gslides tool execution
        print(f"Error calling gslides tool: {e}")
        return {"status": "failure", "error": str(e)}  # Return error status and message


@register_tool("gdocs")
async def gdoc_tool(tool_call: ToolCall) -> Dict[str, Any]:
    """
    GDocs Tool endpoint to handle Google Docs creation and text elaboration using multi-tool support.
    Registered as a tool with the name "gdocs".

    Args:
        tool_call (ToolCall): Request body containing the input for the gdocs tool.

    Returns:
        Dict[str, Any]: A dictionary containing the tool result and tool call string (None in this case).
                         Returns status "failure" and error message if an exception occurs.
    """
    try:
        tool_runnable = get_tool_runnable(  # Initialize gdocs tool runnable
            gdocs_agent_system_prompt_template,
            gdocs_agent_user_prompt_template,
            gdocs_agent_required_format,
            ["query", "previous_tool_response"],  # Expected input parameters
        )
        tool_call_str = tool_runnable.invoke(
            {  # Invoke the gdocs tool runnable
                "query": tool_call["input"],
                "previous_tool_response": tool_call["previous_tool_response"],
            }
        )

        tool_result = await parse_and_execute_tool_calls(
            tool_call_str
        )  # Parse and execute tool calls from the response
        return {
            "tool_result": tool_result,
            "tool_call_str": None,
        }  # Return tool result and None for tool call string
    except Exception as e:  # Handle exceptions during gdocs tool execution
        print(f"Error calling gdocs tool: {e}")
        return {"status": "failure", "error": str(e)}  # Return error status and message


@register_tool("gsheets")
async def gsheet_tool(tool_call: ToolCall) -> Dict[str, Any]:
    """
    GSheets Tool endpoint to handle Google Sheets creation and data population using multi-tool support.
    Registered as a tool with the name "gsheets".

    Args:
        tool_call (ToolCall): Request body containing the input for the gsheets tool.

    Returns:
        Dict[str, Any]: A dictionary containing the tool result and tool call string (None in this case).
                         Returns status "failure" and error message if an exception occurs.
    """

    try:
        tool_runnable = get_tool_runnable(  # Initialize gsheets tool runnable
            gsheets_agent_system_prompt_template,
            gsheets_agent_user_prompt_template,
            gsheets_agent_required_format,
            ["query", "previous_tool_response"],  # Expected input parameters
        )
        tool_call_str = tool_runnable.invoke(
            {  # Invoke the gsheets tool runnable
                "query": tool_call["input"],
                "previous_tool_response": tool_call["previous_tool_response"],
            }
        )

        tool_result = await parse_and_execute_tool_calls(
            tool_call_str
        )  # Parse and execute tool calls from the response
        return {
            "tool_result": tool_result,
            "tool_call_str": None,
        }  # Return tool result and None for tool call string
    except Exception as e:  # Handle exceptions during gsheets tool execution
        print(f"Error calling gsheets tool: {e}")
        return {"status": "failure", "error": str(e)}  # Return error status and message


@register_tool("gcalendar")
async def gcalendar_tool(tool_call: ToolCall) -> Dict[str, Any]:
    """
    GCalendar Tool endpoint to handle Google Calendar interactions using multi-tool support.
    Registered as a tool with the name "gcalendar".

    Args:
        tool_call (ToolCall): Request body containing the input for the gcalendar tool.

    Returns:
        Dict[str, Any]: A dictionary containing the tool result and tool call string (None in this case).
                         Returns status "failure" and error message if an exception occurs.
    """

    try:
        current_time = datetime.now().isoformat()  # Get current time in ISO format
        local_timezone = get_localzone()  # Get local timezone
        timezone = local_timezone.key  # Get timezone key

        tool_runnable = get_tool_runnable(  # Initialize gcalendar tool runnable
            gcalendar_agent_system_prompt_template,
            gcalendar_agent_user_prompt_template,
            gcalendar_agent_required_format,
            [
                "query",
                "current_time",
                "timezone",
                "previous_tool_response",
            ],  # Expected input parameters
        )

        tool_call_str = tool_runnable.invoke(  # Invoke the gcalendar tool runnable
            {
                "query": tool_call["input"],
                "current_time": current_time,
                "timezone": timezone,
                "previous_tool_response": tool_call["previous_tool_response"],
            }
        )

        tool_result = await parse_and_execute_tool_calls(
            tool_call_str
        )  # Parse and execute tool calls from the response
        return {
            "tool_result": tool_result,
            "tool_call_str": None,
        }  # Return tool result and None for tool call string
    except Exception as e:  # Handle exceptions during gcalendar tool execution
        print(f"Error calling gcalendar: {e}")
        return {"status": "failure", "error": str(e)}  # Return error status and message


if __name__ == "__main__":
    # --- Run the application ---
    # This block is executed when the script is run directly (not imported as a module).
    # It starts the uvicorn server to serve the FastAPI application. Note that 5001 is a port used by Sentient in production. Make sure to change it
    uvicorn.run(app, port=5001)

### utils/utils.py ###
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from helpers import *
import uvicorn
import requests
import nest_asyncio
import keyring

from dotenv import load_dotenv

load_dotenv("../.env")  # Load environment variables from .env file

# Load Auth0 configuration from environment variables.
AUTH0_DOMAIN = os.getenv("AUTH0_DOMAIN")
MANAGEMENT_CLIENT_ID = os.getenv("AUTH0_MANAGEMENT_CLIENT_ID")
MANAGEMENT_CLIENT_SECRET = os.getenv("AUTH0_MANAGEMENT_CLIENT_SECRET")

# Initialize FastAPI application.
app = FastAPI(
    docs_url="/docs", 
    redoc_url=None
    )  # Disable default docs and redoc endpoints

# Add CORS middleware to allow cross-origin requests.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)


# Define Pydantic models for request bodies.
class EncryptionRequest(BaseModel):
    """
    Request model for encrypting data.
    """

    data: str


class DecryptionRequest(BaseModel):
    """
    Request model for decrypting data.
    """

    encrypted_data: str


class UserInfoRequest(BaseModel):
    """
    Request model for user information requests.
    """

    user_id: str


class ReferrerStatusRequest(BaseModel):
    """
    Request model for setting referrer status.
    """

    user_id: str
    referrer_status: bool


class BetaUserStatusRequest(BaseModel):
    """
    Request model for setting beta user status.
    """

    user_id: str
    beta_user_status: bool


class SetReferrerRequest(BaseModel):
    """
    Request model for setting referrer based on referral code.
    """

    referral_code: str

class SetApiKeyRequest(BaseModel):
    provider: str
    api_key: str

class HasApiKeyRequest(BaseModel):
    provider: str
    
class DeleteApiKeyRequest(BaseModel):
    provider: str

nest_asyncio.apply()  # Apply nest_asyncio for running async operations in sync context


@app.get("/")
async def main() -> JSONResponse:
    """
    Root endpoint of the API.

    Returns:
        JSONResponse: A simple welcome message.
    """
    return JSONResponse(
        status_code=200,
        content={
            "message": "Hello, I am Sentient, your private, decentralized and interactive AI companion who feels human"
        },
    )


@app.post("/initiate", status_code=200)
async def initiate() -> JSONResponse:
    """
    Endpoint to initiate the model (currently a placeholder).

    Returns:
        JSONResponse: Success or error message indicating initiation status.
    """
    try:
        return JSONResponse(
            status_code=200, content={"message": "Model initiated successfully"}
        )
    except Exception as e:
        return JSONResponse(status_code=500, content={"message": str(e)})


@app.post("/get-role")
async def get_role(request: UserInfoRequest) -> JSONResponse:
    """
    Retrieves the role of a user from Auth0.

    Args:
        request (UserInfoRequest): Request containing the user_id.

    Returns:
        JSONResponse: User's role or error message.
    """
    try:
        management_api_access_token = (
            get_management_token()
        )  # Obtain management API token

        user_id = request.user_id
        roles_response = requests.get(
            f"https://{AUTH0_DOMAIN}/api/v2/users/{user_id}/roles",
            headers={
                "Authorization": f"Bearer {management_api_access_token}"
            },  # Include token in header
        )

        if roles_response.status_code != 200:
            raise HTTPException(
                status_code=roles_response.status_code,
                detail=f"Error fetching user roles: {roles_response.text}",  # Raise HTTP exception if request fails
            )

        roles = roles_response.json()
        if not roles or len(roles) == 0:
            return JSONResponse(
                status_code=404, content={"message": "No roles found for user."}
            )  # Return 404 if no roles are found

        return JSONResponse(
            status_code=200, content={"role": roles[0]["name"].lower()}
        )  # Return the first role name in lowercase

    except Exception as e:
        print(f"Error in get-role: {str(e)}")
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return 500 for any exceptions


@app.post("/get-beta-user-status")
def get_beta_user_status(request: UserInfoRequest) -> JSONResponse:
    """
    Retrieves the beta user status from Auth0 app_metadata.

    Args:
        request (UserInfoRequest): Request containing the user_id.

    Returns:
        JSONResponse: Beta user status or error message.
    """
    try:
        token = get_management_token()  # Obtain management API token
        url = f"https://{AUTH0_DOMAIN}/api/v2/users/{request.user_id}"
        headers = {
            "Authorization": f"Bearer {token}",  # Include token in header
            "Accept": "application/json",
        }

        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Error fetching user info: {response.text}",  # Raise HTTP exception if request fails
            )

        user_data = response.json()
        beta_user_status = user_data.get("app_metadata", {}).get(
            "betaUser"
        )  # Extract betaUser status from app_metadata

        if beta_user_status is None:
            return JSONResponse(
                status_code=404, content={"message": "Beta user status not found."}
            )  # Return 404 if beta user status not found

        return JSONResponse(
            status_code=200, content={"betaUserStatus": beta_user_status}
        )  # Return beta user status
    except Exception as e:
        print(f"Error in beta-user-status: {str(e)}")
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return 500 for any exceptions


@app.post("/get-referral-code")
async def get_referral_code(request: UserInfoRequest) -> JSONResponse:
    """
    Retrieves the referral code from Auth0 app_metadata.

    Args:
        request (UserInfoRequest): Request containing the user_id.

    Returns:
        JSONResponse: Referral code or error message.
    """
    try:
        token = get_management_token()  # Obtain management API token
        url = f"https://{AUTH0_DOMAIN}/api/v2/users/{request.user_id}"
        headers = {
            "Authorization": f"Bearer {token}",  # Include token in header
            "Accept": "application/json",
        }

        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Error fetching user info: {response.text}",  # Raise HTTP exception if request fails
            )

        user_data = response.json()
        referral_code = user_data.get("app_metadata", {}).get(
            "referralCode"
        )  # Extract referralCode from app_metadata
        if not referral_code:
            return JSONResponse(
                status_code=404, content={"message": "Referral code not found."}
            )  # Return 404 if referral code not found

        return JSONResponse(
            status_code=200, content={"referralCode": referral_code}
        )  # Return referral code
    except Exception as e:
        print(f"Error in get-referral-code: {str(e)}")
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return 500 for any exceptions


@app.post("/get-referrer-status")
async def get_referrer_status(request: UserInfoRequest) -> JSONResponse:
    """
    Retrieves the referrer status from Auth0 app_metadata.

    Args:
        request (UserInfoRequest): Request containing the user_id.

    Returns:
        JSONResponse: Referrer status or error message.
    """
    try:
        token = get_management_token()  # Obtain management API token
        url = f"https://{AUTH0_DOMAIN}/api/v2/users/{request.user_id}"
        headers = {
            "Authorization": f"Bearer {token}",  # Include token in header
            "Accept": "application/json",
        }

        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Error fetching user info: {response.text}",  # Raise HTTP exception if request fails
            )

        user_data = response.json()
        referrer_status = user_data.get("app_metadata", {}).get(
            "referrer"
        )  # Extract referrer status from app_metadata
        if referrer_status is None:
            return JSONResponse(
                status_code=404, content={"message": "Referrer status not found."}
            )  # Return 404 if referrer status not found

        return JSONResponse(
            status_code=200, content={"referrerStatus": referrer_status}
        )  # Return referrer status
    except Exception as e:
        print(f"Error in referrer-status: {str(e)}")
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return 500 for any exceptions


@app.post("/set-referrer-status")
async def set_referrer_status(request: ReferrerStatusRequest) -> JSONResponse:
    """
    Sets the referrer status in Auth0 app_metadata.

    Args:
        request (ReferrerStatusRequest): Request containing user_id and referrer_status.

    Returns:
        JSONResponse: Success or error message.
    """
    try:
        token = get_management_token()  # Obtain management API token
        url = f"https://{AUTH0_DOMAIN}/api/v2/users/{request.user_id}"
        headers = {
            "Authorization": f"Bearer {token}",  # Include token in header
            "Content-Type": "application/json",
        }

        payload = {
            "app_metadata": {
                "referrer": request.referrer_status  # Set referrer status in app_metadata
            }
        }

        response = requests.patch(
            url, headers=headers, json=payload
        )  # Use PATCH to update user metadata
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Error updating referrer status: {response.text}",  # Raise HTTP exception if request fails
            )

        return JSONResponse(
            status_code=200,
            content={"message": "Referrer status updated successfully."},
        )  # Return success message
    except Exception as e:
        print(f"Error in set-referrer-status: {str(e)}")
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return 500 for any exceptions


@app.post("/get-user-and-set-referrer-status")
async def get_user_and_set_referrer_status(request: SetReferrerRequest) -> JSONResponse:
    """
    Searches for a user by referral code and sets their referrer status to true.

    Args:
        request (SetReferrerRequest): Request containing referral_code.

    Returns:
        JSONResponse: Success or error message.
    """
    try:
        token = get_management_token()  # Obtain management API token
        headers = {
            "Authorization": f"Bearer {token}",  # Include token in header
            "Accept": "application/json",
            "Content-Type": "application/json",
        }

        search_url = f"https://{AUTH0_DOMAIN}/api/v2/users?q=app_metadata.referralCode%3A%22{request.referral_code}%22"  # Search URL to find user by referral code
        search_response = requests.get(search_url, headers=headers)

        if search_response.status_code != 200:
            raise HTTPException(
                status_code=search_response.status_code,
                detail=f"Error searching for user: {search_response.text}",  # Raise HTTP exception if search fails
            )

        users = search_response.json()

        if not users or len(users) == 0:
            raise HTTPException(
                status_code=404,
                detail=f"No user found with referral code: {request.referral_code}",  # Raise 404 if no user found
            )

        user_id = users[0]["user_id"]  # Get user_id from search results

        referrer_status_payload = {"user_id": user_id, "referrer_status": True}

        set_status_url = f"http://localhost:5005/set-referrer-status"  # URL to set referrer status (assuming local service)
        set_status_response = requests.post(
            set_status_url, json=referrer_status_payload
        )  # Call local service to set referrer status

        if set_status_response.status_code != 200:
            raise HTTPException(
                status_code=set_status_response.status_code,
                detail=f"Error setting referrer status: {set_status_response.text}",  # Raise HTTP exception if setting status fails
            )

        return JSONResponse(
            status_code=200,
            content={"message": "Referrer status updated successfully."},
        )  # Return success message

    except Exception as e:
        print(f"Error in get-user-and-set-referrer-status: {str(e)}")
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return 500 for any exceptions


@app.post("/set-beta-user-status")
def set_beta_user_status(request: BetaUserStatusRequest) -> JSONResponse:
    """
    Sets the beta user status in Auth0 app_metadata.

    Args:
        request (BetaUserStatusRequest): Request containing user_id and beta_user_status.

    Returns:
        JSONResponse: Success or error message.
    """
    try:
        token = get_management_token()  # Obtain management API token
        url = f"https://{AUTH0_DOMAIN}/api/v2/users/{request.user_id}"
        headers = {
            "Authorization": f"Bearer {token}",  # Include token in header
            "Content-Type": "application/json",
        }

        payload = {
            "app_metadata": {
                "betaUser": request.beta_user_status  # Set betaUser status in app_metadata
            }
        }

        response = requests.patch(
            url, headers=headers, json=payload
        )  # Use PATCH to update user metadata
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Error updating beta user status: {response.text}",  # Raise HTTP exception if request fails
            )

        return JSONResponse(
            status_code=200,
            content={"message": "Beta user status updated successfully."},
        )  # Return success message
    except Exception as e:
        print(f"Error in set-beta-user-status: {str(e)}")
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return 500 for any exceptions


@app.post("/get-user-and-invert-beta-user-status")
def get_user_and_invert_beta_user_status(request: UserInfoRequest) -> JSONResponse:
    """
    Searches for a user by user id and inverts the beta user status in Auth0 app_metadata.

    Args:
        request (UserInfoRequest): Request containing user_id.

    Returns:
        JSONResponse: Success or error message.
    """
    try:
        token = get_management_token()  # Obtain management API token
        url = f"https://{AUTH0_DOMAIN}/api/v2/users/{request.user_id}"
        headers = {
            "Authorization": f"Bearer {token}",  # Include token in header
            "Accept": "application/json",
        }

        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Error fetching user info: {response.text}",  # Raise HTTP exception if request fails
            )

        user_data = response.json()
        beta_user_status = user_data.get("app_metadata", {}).get(
            "betaUser"
        )  # Get current betaUser status

        # Invert the beta user status (string boolean to boolean and then invert)
        beta_user_status_payload = {
            "user_id": request.user_id,
            "beta_user_status": False
            if str(beta_user_status).lower() == "true"
            else True,
        }

        set_status_url = f"http://localhost:5005/set-beta-user-status"  # URL to set beta user status (assuming local service)
        set_status_response = requests.post(
            set_status_url, json=beta_user_status_payload
        )  # Call local service to set inverted beta user status

        if set_status_response.status_code != 200:
            raise HTTPException(
                status_code=set_status_response.status_code,
                detail=f"Error inverting beta user status: {set_status_response.text}",  # Raise HTTP exception if setting status fails
            )

        return JSONResponse(
            status_code=200,
            content={"message": "Beta user status inverted successfully."},
        )  # Return success message
    except Exception as e:
        print(f"Error in get-user-and-invert-beta-user-status: {str(e)}")
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return 500 for any exceptions


@app.post("/encrypt")
async def encrypt_data(request: EncryptionRequest) -> JSONResponse:
    """
    Encrypts the provided data using AES encryption.

    Args:
        request (EncryptionRequest): Request containing the data to encrypt.

    Returns:
        JSONResponse: Encrypted data or error message.
    """
    try:
        data = request.data
        encrypted_data = aes_encrypt(
            data
        )  # Encrypt data using aes_encrypt helper function
        return JSONResponse(
            status_code=200, content={"encrypted_data": encrypted_data}
        )  # Return encrypted data
    except Exception as e:
        print(f"Error in encrypt: {str(e)}")
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return 500 for any exceptions


@app.post("/decrypt")
async def decrypt_data(request: DecryptionRequest) -> JSONResponse:
    """
    Decrypts the provided encrypted data using AES decryption.

    Args:
        request (DecryptionRequest): Request containing the encrypted data to decrypt.

    Returns:
        JSONResponse: Decrypted data or error message.
    """
    try:
        data = request.encrypted_data
        decrypted_data = aes_decrypt(
            data
        )  # Decrypt data using aes_decrypt helper function
        return JSONResponse(
            status_code=200, content={"decrypted_data": decrypted_data}
        )  # Return decrypted data
    except Exception as e:
        print(f"Error in decrypt: {str(e)}")
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return 500 for any exceptions

if __name__ == "__main__":
    uvicorn.run(
        app, port=5005
    ) 
### utils/startserv.sh ###
sudo -E <your-venv-path> -m uvicorn utils:app --host 0.0.0.0 --port 5005
### common/common.py ###
import os
import uvicorn
from runnables import *  # Importing runnables (likely Langchain Runnable sequences) from runnables.py
from functions import *  # Importing custom functions from functions.py
from helpers import *  # Importing helper functions from helpers.py
from prompts import *  # Importing prompt templates from prompts.py
import nest_asyncio  # For running asyncio event loop within another event loop (needed for FastAPI in some environments)
from fastapi import FastAPI  # Importing FastAPI for creating the API application
from pydantic import (
    BaseModel,
)  # Importing BaseModel from Pydantic for request body validation and data modeling
from fastapi.responses import (
    JSONResponse,
)  # Importing JSONResponse for sending JSON responses from API endpoints
from fastapi.middleware.cors import (
    CORSMiddleware,
)  # Importing CORSMiddleware to handle Cross-Origin Resource Sharing
from fastapi import FastAPI  # Re-importing FastAPI (likely a typo and redundant)
from pydantic import BaseModel  # Re-importing BaseModel (likely a typo and redundant)
from dotenv import load_dotenv

load_dotenv("../.env")  # Load environment variables from .env file

# --- FastAPI Application Initialization ---
app = FastAPI(
    docs_url="/docs", 
    redoc_url=None
    )  # Creating a FastAPI application instance

# --- CORS Middleware Configuration ---
# Configuring CORS to allow requests from any origin.
# This is generally fine for open-source projects or APIs intended for broad use, but be cautious in production environments.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,  # Allows credentials (cookies, authorization headers) to be included in requests
    allow_methods=["*"],  # Allows all HTTP methods (GET, POST, PUT, DELETE, etc.)
    allow_headers=["*"],  # Allows all headers to be included in requests
)


# --- Pydantic Models for Request Body Validation ---
class InternetSearchRequest(BaseModel):
    """
    Pydantic model for validating internet search requests.
    Requires a 'query' string field.
    """

    query: str  # User's search query string


class ContextClassificationRequest(BaseModel):
    """
    Pydantic model for validating context classification requests.
    Requires a 'query' string and a 'context' string field.
    """

    query: str  # User's query string
    context: str  # Context for classification (e.g., "category", "internet")


class ChatClassificationRequest(BaseModel):
    """
    Pydantic model for validating chat classification requests.
    Requires an 'input' string and a 'chat_id' string field.
    """

    input: str  # User's chat input string
    chat_id: str  # Identifier for the chat session


# --- Global Variables for Runnables and Chat History ---
# These global variables store initialized Langchain Runnable sequences and chat history.
# Initialized in the `/initiate` endpoint.
chat_history = (
    None  # Stores chat history, likely as a list of messages or a database connection
)
chat_runnable = None  # Runnable for general chat interactions (not currently used in the provided endpoints)
orchestrator_runnable = (
    None  # Runnable for orchestrating different tasks based on user input
)
context_classification_runnable = (
    None  # Runnable for classifying the context of a query (e.g., category)
)
internet_search_runnable = None  # Runnable for performing internet searches
internet_query_reframe_runnable = None  # Runnable for reframing internet search queries
internet_summary_runnable = None  # Runnable for summarizing internet search results
orchestrator_runnable = None  # Runnable for orchestrating tasks (redundant declaration)
internet_classification_runnable = (
    None  # Runnable for classifying if a query requires internet search
)

# --- Apply nest_asyncio ---
# Applying nest_asyncio to allow asyncio event loops to be nested.
# This is often needed when running FastAPI applications in environments that may already have an event loop running,
# such as Jupyter notebooks or certain testing frameworks.
nest_asyncio.apply()


# --- API Endpoints ---
@app.get("/")
async def main():
    """
    Root endpoint of the API.
    Returns a simple welcome message.
    """
    return {
        "message": "Hello, I am Sentient, your private, decentralized and interactive AI companion who feels human"
    }


@app.post("/initiate", status_code=200)
async def initiate():
    """
    Endpoint to initialize the AI model and runnables.
    This endpoint sets up the global runnable variables required for other API calls.
    Returns a success message or an error message if initialization fails.
    """
    global \
        chat_history, \
        context_classification_runnable, \
        internet_search_runnable, \
        internet_query_reframe_runnable, \
        internet_summary_runnable, \
        internet_classification_runnable, \
        orchestrator_runnable

    # Initialize different runnables using functions from runnables.py
    context_classification_runnable = (
        get_context_classification_runnable()
    )  # Get runnable for context classification
    internet_classification_runnable = (
        get_internet_classification_runnable()
    )  # Get runnable for internet classification
    internet_search_runnable = get_internet_classification_runnable()  # Get runnable for internet search (typo in original code, should likely be `get_internet_search_runnable()`)
    internet_query_reframe_runnable = (
        get_internet_query_reframe_runnable()
    )  # Get runnable for reframing internet queries
    internet_summary_runnable = (
        get_internet_summary_runnable()
    )  # Get runnable for summarizing internet search results

    try:
        return JSONResponse(
            status_code=200, content={"message": "Model initiated successfully"}
        )  # Return success message
    except Exception as e:
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return error message if exception occurs


@app.post("/chat-classify", status_code=200)
async def chat_classify(request: ChatClassificationRequest):
    """
    Endpoint to classify user chat input and determine the appropriate response strategy.
    Uses an orchestrator runnable to classify the input and potentially transform it.
    Returns the classification and transformed input.
    """
    try:
        global orchestrator_runnable, chat_history

        chat_history = get_chat_history()
        orchestrator_runnable = get_orchestrator_runnable(
            chat_history
        )  # Initialize orchestrator runnable with chat history
        orchestrator_output = orchestrator_runnable.invoke(
            {"query": request.input}
        )  # Invoke orchestrator runnable with user input
        classification = orchestrator_output[
            "class"
        ]  # Extract classification from orchestrator output
        transformed_input = orchestrator_output[
            "input"
        ]  # Extract transformed input from orchestrator output

        return JSONResponse(
            status_code=200,
            content={
                "classification": classification,
                "transformed_input": transformed_input,
            },
        )  # Return classification and transformed input
    except Exception as e:
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return error message if exception occurs


@app.post("/context-classify", status_code=200)
async def context_classify(request: ContextClassificationRequest):
    """
    Endpoint to classify user query based on the provided context.
    Uses different classification runnables based on the context.
    Returns the classification result.
    """
    try:
        global context_classification_runnable, internet_classification_runnable

        if request.context == "category":  # Classify based on category context
            classification = context_classification_runnable.invoke(
                {"query": request.query}
            )  # Invoke context classification runnable
        else:  # Classify based on internet context (or default to internet classification if context is not "category")
            classification = internet_classification_runnable.invoke(
                {"query": request.query}
            )  # Invoke internet classification runnable

        return JSONResponse(
            status_code=200, content={"classification": classification}
        )  # Return classification result
    except Exception as e:
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return error message if exception occurs


@app.post("/internet-search", status_code=200)
async def internet_search(request: InternetSearchRequest):
    """
    Endpoint to perform an internet search based on the user query.
    Reframes the query, fetches search results, and summarizes them to provide internet context.
    Returns the summarized internet context.
    """
    try:
        reframed_query = get_reframed_internet_query(
            internet_query_reframe_runnable, request.query
        )  # Reframe the user query for better internet search
        search_results = get_search_results(
            reframed_query
        )  # Fetch search results using the reframed query
        internet_context = get_search_summary(
            internet_summary_runnable, search_results
        )  # Summarize the search results to get internet context

        return JSONResponse(
            status_code=200, content={"internet_context": internet_context}
        )  # Return the summarized internet context

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"message": f"Error performing internet search: {str(e)}"},
        )  # Return error message if exception occurs


# --- Main execution block ---
if __name__ == "__main__":
    """
    This block is executed when the script is run directly (not imported as a module).
    It starts the uvicorn server to run the FastAPI application.
    """
    uvicorn.run(app, port=5006)  # Start uvicorn server on port 5006

### common/startserv.sh ###
sudo -E <your-venv-path> -m uvicorn common:app --host 0.0.0.0 --port 5006
### chat/chat.py ###
import os
import uvicorn
import json
import asyncio
from runnables import *
from functions import *
from externals import *
from helpers import *
from prompts import *
import nest_asyncio
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from typing import Dict, Any, AsyncGenerator
from dotenv import load_dotenv
import time

load_dotenv("../.env")  # Load environment variables from .env file

# --- FastAPI Application ---
app = FastAPI(
    title="Chat API", description="API for chat functionalities",
    docs_url="/docs",
    redoc_url=None
)

# --- CORS Middleware ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models for Request Bodies ---
class Message(BaseModel):
    """
    Pydantic model for the chat message request body.
    """
    original_input: str
    transformed_input: str
    pricing: str
    credits: int
    chat_id: str

# --- Global Variables and Database Setup ---
db_path = os.path.join(os.path.dirname(__file__), "..", "..", "chatsDb.json")
db_lock = asyncio.Lock()  # Lock for synchronizing database access

def load_db():
    """Load the database from chatsDb.json, initializing with {"messages": []} if it doesn't exist or is invalid."""
    try:
        with open(db_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        print ("DB NOT FOUND!")
        return {"messages": []}

def save_db(data):
    """Save the data to chatsDb.json."""
    with open(db_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4)

db_data = load_db()  # Load database into memory at startup
chat_runnable = None  # Global chat runnable, initialized later

# --- Apply nest_asyncio ---
nest_asyncio.apply()

# --- API Endpoints ---
@app.get("/", status_code=200)
async def main() -> Dict[str, str]:
    """Root endpoint of the Chat API."""
    return {
        "message": "Hello, I am Sentient, your private, decentralized and interactive AI companion who feels human"
    }

@app.post("/initiate", status_code=200)
async def initiate() -> JSONResponse:
    """Endpoint to initiate the Chat API model."""
    try:
        return JSONResponse(
            status_code=200, content={"message": "Model initiated successfully"}
        )
    except Exception as e:
        print(f"Error initiating chat: {e}")
        return JSONResponse(status_code=500, content={"message": str(e)})

@app.get("/get-chat-history", status_code=200)
async def get_chat_history():
    """Retrieve the chat history."""
    async with db_lock:
        return JSONResponse(status_code=200, content={"messages": db_data["messages"]})

@app.post("/clear-chat-history", status_code=200)
async def clear_chat_history():
    """Clear the chat history."""
    async with db_lock:
        db_data["messages"] = []
        save_db(db_data)
    return JSONResponse(status_code=200, content={"message": "Chat history cleared"})

@app.post("/chat", status_code=200)
async def chat(message: Message):
    """Handle chat interactions with streaming responses."""
    global chat_runnable
    try:
        with open("../../userProfileDb.json", "r", encoding="utf-8") as f:
            user_db = json.load(f)
        
        chat_history = get_formatted_chat_history()  # Get raw messages list
        chat_runnable = get_chat_runnable(chat_history)

        username = user_db["userData"]["personalInfo"]["name"]
        transformed_input = message.transformed_input
        pricing_plan = message.pricing
        credits = message.credits

        async def response_generator():
            memory_used = False
            agents_used = False
            internet_used = False
            user_context = None
            internet_context = None
            pro_used = False
            note = ""

            # Save user message
            user_msg = {
                "id": str(int(time.time() * 1000)),
                "message": message.original_input,
                "isUser": True,
                "memoryUsed": False,
                "agentsUsed": False,
                "internetUsed": False
            }
            async with db_lock:
                db_data["messages"].append(user_msg)
                save_db(db_data)

            yield json.dumps({
                "type": "userMessage",
                "message": message.original_input,
                "memoryUsed": False,
                "agentsUsed": False,
                "internetUsed": False
            }) + "\n"
            await asyncio.sleep(0.05)

            yield json.dumps({"type": "intermediary", "message": "Processing chat response..."}) + "\n"
            await asyncio.sleep(0.05)

            context_classification = await classify_context(transformed_input, "category")
            if "personal" in context_classification["class"]:
                yield json.dumps({"type": "intermediary", "message": "Retrieving memories..."}) + "\n"
                memory_used = True
                user_context = await perform_graphrag(transformed_input)
                
            note = ""
            internet_classification = await classify_context(transformed_input, "internet")
            if pricing_plan == "free" and internet_classification["class"] == "Internet" and credits > 0:
                yield json.dumps({"type": "intermediary", "message": "Searching the internet..."}) + "\n"
                internet_context = await perform_internet_search(transformed_input)
                internet_used = True
                pro_used = True
            elif pricing_plan == "free" and internet_classification["class"] != "Internet":
                pass
            elif pricing_plan != "free" and internet_classification["class"] == "Internet":
                yield json.dumps({"type": "intermediary", "message": "Searching the internet..."}) + "\n"
                internet_context = await perform_internet_search(transformed_input)
                internet_used = True
                pro_used = True
            else:
                note = "Sorry, internet search is a pro feature and requires credits on the free plan."

            personality = user_db["userData"].get("personality", "None")
            assistant_msg = {
                "id": str(int(time.time() * 1000)),
                "message": "",
                "isUser": False,
                "memoryUsed": memory_used,
                "agentsUsed": agents_used,
                "internetUsed": internet_used
            }
            async with db_lock:
                db_data["messages"].append(assistant_msg)
                save_db(db_data)

            async for token in generate_streaming_response(
                chat_runnable,
                inputs={
                    "query": transformed_input,
                    "user_context": user_context,
                    "internet_context": internet_context,
                    "name": username,
                    "personality": personality
                },
                stream=True
            ):
                if isinstance(token, str):
                    assistant_msg["message"] += token
                    async with db_lock:
                        save_db(db_data)
                    yield json.dumps({
                        "type": "assistantStream",
                        "token": token,
                        "done": False,
                        "messageId": assistant_msg["id"]
                    }) + "\n"
                    await asyncio.sleep(0.05)
                else:
                    # Streaming is done, append the full assistant message
                    if note:
                        assistant_msg["message"] += "\n\n" + note
                    chat_runnable.messages.append({"role": "assistant", "content": assistant_msg["message"]})
                    async with db_lock:
                        save_db(db_data)
                    yield json.dumps({
                        "type": "assistantStream",
                        "token": "\n\n" + note if note else "",
                        "done": True,
                        "memoryUsed": memory_used,
                        "agentsUsed": agents_used,
                        "internetUsed": internet_used,
                        "proUsed": pro_used,
                        "messageId": assistant_msg["id"]
                    }) + "\n"

        return StreamingResponse(response_generator(), media_type="application/json")
    except Exception as e:
        print(f"Error executing chat: {e}")
        return JSONResponse(status_code=500, content={"message": str(e)})

if __name__ == "__main__":
    uvicorn.run(app, port=5003)
### chat/startserv.sh ###
sudo -E <your-venv-path> -m uvicorn chat:app --host 0.0.0.0 --port 5003
### app/startserv.sh ###
sudo -E <your-venv-path> -m uvicorn app:app --host 0.0.0.0 --port 5000
### app/app.py ###
import os
import sys
import httpx
import time
from typing import Union
import uvicorn
import asyncio
import psutil
import subprocess
import json
from helpers import *
import nest_asyncio
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from typing import Dict, Optional, List, Any, AsyncGenerator
import multiprocessing
import traceback
import sys
from dotenv import load_dotenv

load_dotenv("../.env")  # Load environment variables from .env file

# --- Service Configuration ---
# Mapping of service categories to their respective port environment variables.
CATEGORY_SERVERS: Dict[str, str] = {
    "chat": os.getenv("CHAT_SERVER_PORT"),
    "memory": os.getenv("MEMORY_SERVER_PORT"),
    "agents": os.getenv("AGENTS_SERVER_PORT"),
    "scraper": os.getenv("SCRAPER_SERVER_PORT"),
    "utils": os.getenv("UTILS_SERVER_PORT"),
    "common": os.getenv("COMMON_SERVER_PORT"),
}

# Mapping of port environment variables to their executable file names (Windows .exe files).
SERVICE_MODULES: Dict[str, str] = {
    "AGENTS_SERVER_PORT": "agents.exe",
    "MEMORY_SERVER_PORT": "memory.exe",
    "CHAT_SERVER_PORT": "chat.exe",
    "SCRAPER_SERVER_PORT": "scraper.exe",
    "UTILS_SERVER_PORT": "utils.exe",
    "COMMON_SERVER_PORT": "common.exe",
}

initiated_services: Dict[str, bool] = {}

# --- FastAPI Application ---
app = FastAPI(
    title="Orchestrator API",
    description="Orchestrates different services to provide a seamless AI experience.",
    docs_url="/docs", 
    redoc_url=None
)  # Initialize FastAPI application

# --- CORS Middleware ---
# Configure CORS to allow cross-origin requests.
# In a production environment, you should restrict the `allow_origins` to specific domains for security.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins - configure this for production
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods - configure this for production
    allow_headers=["*"],  # Allows all headers - configure this for production
)

# --- Pydantic Models for Request Bodies ---
# Define Pydantic models for request validation and data structure.


class Message(BaseModel):
    """BaseModel for chat messages."""

    input: str
    pricing: str
    credits: int
    chat_id: str


class ChatId(BaseModel):
    """BaseModel for chat IDs."""

    id: str


class InternetSearchRequest(BaseModel):
    """BaseModel for internet search requests."""

    query: str


class ContextClassificationRequest(BaseModel):
    """BaseModel for context classification requests."""

    query: str
    context: str


class ElaboratorMessage(BaseModel):
    """BaseModel for elaborator messages."""

    input: str
    purpose: str


class DeleteSubgraphRequest(BaseModel):
    """BaseModel for delete subgraph requests."""

    source: str


class GraphRequest(BaseModel):
    """BaseModel for generic graph requests."""

    information: str


class RedditURL(BaseModel):
    """BaseModel for Reddit URL requests."""

    url: str


class TwitterURL(BaseModel):
    """BaseModel for Twitter URL requests."""

    url: str


class Profile(BaseModel):
    """BaseModel for social media profile URLs."""

    url: str


class EncryptionRequest(BaseModel):
    """BaseModel for encryption requests."""

    data: str


class DecryptionRequest(BaseModel):
    """BaseModel for decryption requests."""

    encrypted_data: str


class UserInfoRequest(BaseModel):
    """BaseModel for user info requests (user_id)."""

    user_id: str


class ReferrerStatusRequest(BaseModel):
    """BaseModel for referrer status update requests."""

    user_id: str
    referrer_status: bool


class SetReferrerRequest(BaseModel):
    """BaseModel for setting referrer using referral code."""

    referral_code: str


class GraphRAGRequest(BaseModel):
    """BaseModel for GraphRAG (Graph-based Retrieval Augmented Generation) requests."""

    query: str


class InternetSearchRequest(BaseModel):
    """BaseModel for internet search requests."""

    query: str


class ContextClassificationRequest(BaseModel):
    """BaseModel for context classification requests."""

    query: str
    context: str


class BetaUserStatusRequest(BaseModel):
    """BaseModel for beta user status update requests."""

    user_id: str
    beta_user_status: bool


# --- Global Variables ---
# Global variables to maintain chat context and runnables.
chat_id: Optional[str] = (
    None  # Global variable to store the current chat ID, initialized to None
)
chat_history = (
    None  # Placeholder for chat history object, currently unused in this orchestrator
)
chat_runnable = (
    None  # Placeholder for chat runnable, currently unused in this orchestrator
)
orchestrator_runnable = None  # Placeholder for orchestrator runnable, currently unused
context_classification_runnable = (
    None  # Placeholder for context classification runnable
)
internet_search_runnable = None  # Placeholder for internet search runnable
internet_query_reframe_runnable = (
    None  # Placeholder for internet query reframe runnable
)
internet_summary_runnable = None  # Placeholder for internet summary runnable


# --- Asyncio Integration ---
nest_asyncio.apply()  # Apply nest_asyncio to allow nested asyncio event loops


# --- API Endpoints ---
# Define FastAPI endpoints for the orchestrator service.


@app.get("/", status_code=200)
async def main() -> Dict[str, str]:
    """
    Root endpoint of the orchestrator API.

    Returns:
        JSONResponse: A simple greeting message in JSON format.
    """
    return {
        "message": "Hello, I am Sentient, your private, decentralized and interactive AI companion who feels human"
    }  # Return a greeting message
   
# @app.get("/get-chat-history", status_code=200)
# async def get_chat_history():
#     return await call_service_endpoint("CHAT_SERVER_PORT", "/get-chat-history", method="GET")

# @app.post("/clear-chat-history", status_code=200)
# async def clear_chat_history():
#     return await call_service_endpoint("CHAT_SERVER_PORT", "/clear-chat-history", method="POST")

# @app.post("/elaborator", status_code=200)
# async def elaborate(message: ElaboratorMessage) -> JSONResponse:
#     """
#     Endpoint to proxy elaboration requests to the Agent Service.

#     Forwards elaboration requests to the Agent Service and returns its response.
#     The elaborator service is now integrated within the Agent Service.

#     Args:
#         message (ElaboratorMessage): Request body containing the input for elaboration.

#     Returns:
#         JSONResponse: Response from the Agent Service's elaborator endpoint.
#     """
#     payload: Dict[str, str] = (
#         message.model_dump()
#     )  # Extract payload from ElaboratorMessage model
#     return await call_service_endpoint(
#         "AGENTS_SERVER_PORT", "/elaborator", method="POST", payload= payload
#     )  # Call Agent Service elaborator endpoint


# @app.post("/create-graph", status_code=200)
# async def create_graph() -> JSONResponse:
#     """
#     Endpoint to proxy graph creation requests to the Memory Service.

#     Forwards requests to create a new graph to the Memory Service and returns its response.
#     The graph creation service is now part of the Memory Service.

#     Returns:
#         JSONResponse: Response from the Memory Service's create-graph endpoint.
#     """
#     return await call_service_endpoint(
#         "MEMORY_SERVER_PORT", "/create-graph", method="POST"
#     )  # Call Memory Service create-graph endpoint


# @app.post("/delete-subgraph", status_code=200)
# async def delete_subgraph(request: DeleteSubgraphRequest) -> JSONResponse:
#     """
#     Endpoint to proxy subgraph deletion requests to the Memory Service.

#     Forwards requests to delete a subgraph to the Memory Service and returns its response.
#     The delete subgraph service is part of the Memory Service.

#     Args:
#         request (DeleteSubgraphRequest): Request body specifying the source of the subgraph to delete.

#     Returns:
#         JSONResponse: Response from the Memory Service's delete-subgraph endpoint.
#     """
#     payload: Dict[str, str] = (
#         request.model_dump()
#     )  # Extract payload from DeleteSubgraphRequest model
#     return await call_service_endpoint(
#         "MEMORY_SERVER_PORT", "/delete-subgraph", method="POST", payload= payload
#     )  # Call Memory Service delete-subgraph endpoint


# @app.post("/create-document", status_code=200)
# async def create_document() -> JSONResponse:
#     """
#     Endpoint to proxy document creation requests to the Memory Service.

#     Forwards requests to create a new document to the Memory Service and returns its response.
#     The create document service is now part of the Memory Service.

#     Returns:
#         JSONResponse: Response from the Memory Service's create-document endpoint.
#     """
#     return await call_service_endpoint(
#         "MEMORY_SERVER_PORT", "/create-document", method="POST"
#     )  # Call Memory Service create-document endpoint


# @app.post("/scrape-linkedin", status_code=200)
# async def scrape_linkedin(profile: Profile) -> JSONResponse:
#     """
#     Endpoint to proxy LinkedIn scraping requests to the Scraper Service.

#     Forwards requests to scrape LinkedIn profiles to the Scraper Service and returns its response.

#     Args:
#         profile (Profile): Request body containing the LinkedIn profile URL to scrape.

#     Returns:
#         JSONResponse: Response from the Scraper Service's scrape-linkedin endpoint.
#     """
#     payload: Dict[str, str] = profile.model_dump()  # Extract payload from Profile model
#     return await call_service_endpoint(
#         "SCRAPER_SERVER_PORT", "/scrape-linkedin", method="POST", payload= payload
#     )  # Call Scraper Service scrape-linkedin endpoint


# @app.post("/customize-graph", status_code=200)
# async def customize_graph(request: GraphRequest) -> JSONResponse:
#     """
#     Endpoint to proxy graph customization requests to the Memory Service.

#     Forwards requests to customize the graph to the Memory Service and returns its response.
#     The customize graph service is now part of the Memory Service.

#     Args:
#         request (GraphRequest): Request body containing information for graph customization.

#     Returns:
#         JSONResponse: Response from the Memory Service's customize-graph endpoint.
#     """
#     payload: Dict[str, str] = (
#         request.model_dump()
#     )  # Extract payload from GraphRequest model
#     return await call_service_endpoint(
#         "MEMORY_SERVER_PORT", "/customize-graph", method="POST", payload= payload
#     )  # Call Memory Service customize-graph endpoint


# @app.post("/scrape-reddit")
# async def scrape_reddit(reddit_url: RedditURL) -> JSONResponse:
#     """
#     Endpoint to proxy Reddit scraping requests to the Scraper Service.

#     Forwards requests to scrape Reddit URLs to the Scraper Service and returns its response.

#     Args:
#         reddit_url (RedditURL): Request body containing the Reddit URL to scrape.

#     Returns:
#         JSONResponse: Response from the Scraper Service's scrape-reddit endpoint.
#     """
#     payload: Dict[str, str] = (
#         reddit_url.model_dump()
#     )  # Extract payload from RedditURL model
#     return await call_service_endpoint(
#         "SCRAPER_SERVER_PORT", "/scrape-reddit", method="POST", payload= payload
#     )  # Call Scraper Service scrape-reddit endpoint


# @app.post("/scrape-twitter")
# async def scrape_twitter(twitter_url: TwitterURL) -> JSONResponse:
#     """
#     Endpoint to proxy Twitter scraping requests to the Scraper Service.

#     Forwards requests to scrape Twitter URLs to the Scraper Service and returns its response.

#     Args:
#         twitter_url (TwitterURL): Request body containing the Twitter URL to scrape.

#     Returns:
#         JSONResponse: Response from the Scraper Service's scrape-twitter endpoint.
#     """
#     payload: Dict[str, str] = (
#         twitter_url.model_dump()
#     )  # Extract payload from TwitterURL model
#     return await call_service_endpoint(
#         "SCRAPER_SERVER_PORT", "/scrape-twitter", method="POST", payload= payload
#     )  # Call Scraper Service scrape-twitter endpoint


# @app.post("/get-role")
# async def get_role_endpoint(request: UserInfoRequest) -> JSONResponse:
#     """
#     Endpoint to proxy user role retrieval requests to the Utils Service.

#     Forwards requests to get user roles to the Utils Service and returns its response.

#     Args:
#         request (UserInfoRequest): Request body containing the user ID for role retrieval.

#     Returns:
#         JSONResponse: Response from the Utils Service's get-role endpoint.
#     """
#     payload: Dict[str, str] = (
#         request.model_dump()
#     )  # Extract payload from UserInfoRequest model
#     return await call_service_endpoint(
#         "UTILS_SERVER_PORT", "/get-role", method="POST", payload= payload
#     )  # Call Utils Service get-role endpoint


# @app.post("/get-referral-code")
# async def get_referral_code_endpoint(request: UserInfoRequest) -> JSONResponse:
#     """
#     Endpoint to proxy referral code retrieval requests to the Utils Service.

#     Forwards requests to get referral codes to the Utils Service and returns its response.

#     Args:
#         request (UserInfoRequest): Request body containing the user ID for referral code retrieval.

#     Returns:
#         JSONResponse: Response from the Utils Service's get-referral-code endpoint.
#     """
#     payload: Dict[str, str] = (
#         request.model_dump()
#     )  # Extract payload from UserInfoRequest model
#     return await call_service_endpoint(
#         "UTILS_SERVER_PORT", "/get-referral-code", method="POST", payload= payload
#     )  # Call Utils Service get-referral-code endpoint


# @app.post("/get-referrer-status")
# async def get_referrer_status_endpoint(request: UserInfoRequest) -> JSONResponse:
#     """
#     Endpoint to proxy referrer status retrieval requests to the Utils Service.

#     Forwards requests to get referrer status to the Utils Service and returns its response.

#     Args:
#         request (UserInfoRequest): Request body containing the user ID for referrer status retrieval.

#     Returns:
#         JSONResponse: Response from the Utils Service's get-referrer-status endpoint.
#     """
#     payload: Dict[str, bool] = (
#         request.model_dump()
#     )  # Extract payload from UserInfoRequest model
#     return await call_service_endpoint(
#         "UTILS_SERVER_PORT", "/get-referrer-status", method="POST", payload= payload
#     )  # Call Utils Service get-referrer-status endpoint


# @app.post("/set-referrer-status")
# async def set_referrer_status_endpoint(request: ReferrerStatusRequest) -> JSONResponse:
#     """
#     Endpoint to proxy referrer status update requests to the Utils Service.

#     Forwards requests to set referrer status to the Utils Service and returns its response.

#     Args:
#         request (ReferrerStatusRequest): Request body containing user ID and new referrer status.

#     Returns:
#         JSONResponse: Response from the Utils Service's set-referrer-status endpoint.
#     """
#     payload: Dict[str, Union[str, bool]] = (
#         request.model_dump()
#     )  # Extract payload from ReferrerStatusRequest model
#     return await call_service_endpoint(
#         "UTILS_SERVER_PORT", "/set-referrer-status", method="POST", payload= payload
#     )  # Call Utils Service set-referrer-status endpoint


# @app.post("/get-user-and-set-referrer-status")
# async def get_user_and_set_referrer_status_endpoint(
#     request: SetReferrerRequest,
# ) -> JSONResponse:
#     """
#     Endpoint to proxy user retrieval and referrer status setting requests to the Utils Service.

#     Forwards requests to get user info and set referrer status based on referral code to the Utils Service and returns its response.

#     Args:
#         request (SetReferrerRequest): Request body containing referral code to set referrer status.

#     Returns:
#         JSONResponse: Response from the Utils Service's get-user-and-set-referrer-status endpoint.
#     """
#     payload: Dict[str, str] = (
#         request.model_dump()
#     )  # Extract payload from SetReferrerRequest model
#     return await call_service_endpoint(
#         "UTILS_SERVER_PORT", "/get-user-and-set-referrer-status", method="POST", payload= payload
#     )  # Call Utils Service get-user-and-set-referrer-status endpoint


# @app.post("/get-user-and-invert-beta-user-status")
# async def get_user_and_invert_beta_user_status_endpoint(
#     request: UserInfoRequest,
# ) -> JSONResponse:
#     """
#     Endpoint to proxy user retrieval and beta user status inversion requests to the Utils Service.

#     Forwards requests to get user info and invert beta user status to the Utils Service and returns its response.

#     Args:
#         request (UserInfoRequest): Request body containing user ID for beta user status inversion.

#     Returns:
#         JSONResponse: Response from the Utils Service's get-user-and-invert-beta-user-status endpoint.
#     """
#     payload: Dict[str, str] = (
#         request.model_dump()
#     )  # Extract payload from UserInfoRequest model
#     return await call_service_endpoint(
#         "UTILS_SERVER_PORT", "/get-user-and-invert-beta-user-status", method="POST", payload= payload
#     )  # Call Utils Service get-user-and-invert-beta-user-status endpoint


# @app.post("/set-beta-user-status")
# async def set_beta_user_status_endpoint(request: BetaUserStatusRequest) -> JSONResponse:
#     """
#     Endpoint to proxy beta user status update requests to the Utils Service.

#     Forwards requests to set beta user status to the Utils Service and returns its response.

#     Args:
#         request (BetaUserStatusRequest): Request body containing user ID and new beta user status.

#     Returns:
#         JSONResponse: Response from the Utils Service's set-beta-user-status endpoint.
#     """
#     payload: Dict[str, Union[str, bool]] = (
#         request.model_dump()
#     )  # Extract payload from BetaUserStatusRequest model
#     return await call_service_endpoint(
#         "UTILS_SERVER_PORT", "/set-beta-user-status", method="POST", payload= payload
#     )  # Call Utils Service set-beta-user-status endpoint


# @app.post("/get-beta-user-status")
# async def get_beta_user_status_endpoint(request: UserInfoRequest) -> JSONResponse:
#     """
#     Endpoint to proxy beta user status retrieval requests to the Utils Service.

#     Forwards requests to get beta user status to the Utils Service and returns its response.

#     Args:
#         request (UserInfoRequest): Request body containing user ID for beta user status retrieval.

#     Returns:
#         JSONResponse: Response from the Utils Service's get-beta-user-status endpoint.
#     """
#     payload: Dict[str, str] = (
#         request.model_dump()
#     )  # Extract payload from UserInfoRequest model
#     return await call_service_endpoint(
#         "UTILS_SERVER_PORT", "/get-beta-user-status", method="POST", payload=payload
#     )  # Call Utils Service get-beta-user-status endpoint


# @app.post("/encrypt")
# async def encrypt_endpoint(request: EncryptionRequest) -> JSONResponse:
#     """
#     Endpoint to proxy encryption requests to the Utils Service.

#     Forwards requests to encrypt data to the Utils Service and returns its response.

#     Args:
#         request (EncryptionRequest): Request body containing data to be encrypted.

#     Returns:
#         JSONResponse: Response from the Utils Service's encrypt endpoint.
#     """
#     payload: Dict[str, str] = (
#         request.model_dump()
#     )  # Extract payload from EncryptionRequest model
#     return await call_service_endpoint(
#         "UTILS_SERVER_PORT", "/encrypt", method="POST", payload=payload
#     )  # Call Utils Service encrypt endpoint


# @app.post("/decrypt")
# async def decrypt_endpoint(request: DecryptionRequest) -> JSONResponse:
#     """
#     Endpoint to proxy decryption requests to the Utils Service.

#     Forwards requests to decrypt data to the Utils Service and returns its response.

#     Args:
#         request (DecryptionRequest): Request body containing encrypted data to be decrypted.

#     Returns:
#         JSONResponse: Response from the Utils Service's decrypt endpoint.
#     """
#     payload: Dict[str, str] = request.model_dump()  # Extract payload from DecryptionRequest model
#     return await call_service_endpoint("UTILS_SERVER_PORT", "/decrypt", method="POST", payload=payload)


# @app.post("/graphrag")
# async def graphrag_endpoint(request: GraphRAGRequest) -> JSONResponse:
#     """
#     Endpoint to proxy GraphRAG requests to the Memory Service.

#     Forwards requests for graph-based retrieval-augmented generation to the Memory Service and returns its streaming response.

#     Args:
#         request (GraphRAGRequest): Request body containing the query for GraphRAG.

#     Returns:
#         StreamingResponse: Streaming response from the Memory Service's graphrag endpoint.
#     """
#     payload: Dict[str, str] = (
#         request.model_dump()
#     )  # Extract payload from GraphRAGRequest model
#     return await call_service_endpoint(
#         "MEMORY_SERVER_PORT", "/graphrag", method="POST", payload= payload
#     )  # Fetch and return streaming response


# @app.post("/internet-search")
# async def internet_search_endpoint(request: InternetSearchRequest) -> JSONResponse:
#     """
#     Endpoint to proxy internet search requests to the Common Service.

#     Forwards requests for internet searches to the Common Service and returns its response.

#     Args:
#         request (InternetSearchRequest): Request body containing the query for internet search.

#     Returns:
#         JSONResponse: Response from the Common Service's internet-search endpoint.
#     """
#     payload: Dict[str, str] = (
#         request.model_dump()
#     )  # Extract payload from InternetSearchRequest model
#     return await call_service_endpoint(
#         "COMMON_SERVER_PORT", "/internet-search", method="POST", payload= payload
#     )  # Call Common Service internet-search endpoint


# @app.post("/context-classify")
# async def context_classify_endpoint(
#     request: ContextClassificationRequest,
# ) -> JSONResponse:
#     """
#     Endpoint to proxy context classification requests to the Common Service.

#     Forwards requests to classify context to the Common Service and returns its response.

#     Args:
#         request (ContextClassificationRequest): Request body containing query and context for classification.

#     Returns:
#         JSONResponse: Response from the Common Service's context-classify endpoint.
#     """
#     payload: Dict[str, str] = (
#         request.model_dump()
#     )  # Extract payload from ContextClassificationRequest model
#     return await call_service_endpoint(
#         "COMMON_SERVER_PORT", "/context-classify", method="POST", payload= payload
#     )  # Call Common Service context-classify endpoint


# @app.post("/chat", status_code=200)
# async def chat(message: Message):
#     try:
#         # Step 1: Classify the chat input
#         response = await call_service_endpoint(
#             "COMMON_SERVER_PORT", "/chat-classify", method="POST",
#             payload={"input": message.input, "chat_id": "single_chat"}
#         )
#         if response.status_code != 200:
#             raise HTTPException(status_code=response.status_code, detail=f"Chat classification failed: {json.loads(response.body)['message']}")

#         response_data = json.loads(response.body)
#         category = response_data["classification"]
#         transformed_input = response_data["transformed_input"]

#         # Step 2: Route to appropriate service
#         category_port_env_var = {"chat": "CHAT_SERVER_PORT", "memory": "MEMORY_SERVER_PORT", "agent": "AGENTS_SERVER_PORT"}.get(category)
#         if not category_port_env_var:
#             raise HTTPException(status_code=400, detail="Invalid category determined by orchestrator")

#         category_port = os.environ.get(category_port_env_var)
#         if not category_port:
#             raise HTTPException(status_code=500, detail=f"{category_port_env_var} not configured.")

#         category_url = f"http://localhost:{category_port}/chat"
#         payload = {
#             "chat_id": "single_chat",
#             "original_input": message.input,
#             "transformed_input": transformed_input,
#             "pricing": message.pricing,
#             "credits": message.credits
#         }
#         return await fetch_streaming_response(category_url, payload)
#     except HTTPException as http_exc:
#         return JSONResponse(status_code=http_exc.status_code, content={"message": http_exc.detail})
#     except Exception as e:
#         print(f"Error in chat endpoint: {e}")
#         return JSONResponse(status_code=500, content={"message": f"Error processing chat: {str(e)}"})
    
# # --- Server Shutdown Endpoint ---
# # Endpoint to gracefully stop backend services when the orchestrator app closes.

# SERVERS_TO_STOP_ON_APP_CLOSE: List[str] = [
#     "AGENTS_SERVER_PORT",
#     "SCRAPER_SERVER_PORT",
#     "MEMORY_SERVER_PORT",
#     "CHAT_SERVER_PORT",
#     "UTILS_SERVER_PORT",
#     "COMMON_SERVER_PORT",
# ]  # List of servers to stop on app close


# @app.post("/stop-servers-on-app-close")
# async def stop_servers_endpoint() -> JSONResponse:
#     """
#     Endpoint to stop specific backend servers when the orchestrator application is closing.

#     This endpoint iterates through a predefined list of server port environment variables,
#     stopping each server asynchronously. It aggregates the results and returns a JSON response
#     indicating the success or failure of stopping each server.

#     Returns:
#         JSONResponse: A FastAPI JSONResponse object indicating the status of stopping each server.
#                       Returns a success message if all servers are stopped successfully,
#                       or an error message if some servers failed to stop.
#     """
#     results: Dict[
#         str, str
#     ] = {}  # Initialize dictionary to store results of stopping each server

#     try:
#         for (
#             server
#         ) in SERVERS_TO_STOP_ON_APP_CLOSE:  # Iterate through list of servers to stop
#             result: bool = await stop_server(server)  # Stop each server asynchronously
#             results[server] = (
#                 "Stopped" if result else "Failed"
#             )  # Record stop status in results dictionary

#         if all(
#             status == "Stopped" for status in results.values()
#         ):  # Check if all servers were stopped successfully
#             print("All servers stopped successfully")
#             return JSONResponse(
#                 status_code=200, content={"message": "All servers stopped successfully"}
#             )  # Return success response if all servers stopped
#         else:  # If not all servers stopped successfully
#             print("Some servers failed to stop")
#             return JSONResponse(
#                 status_code=500, content={"message": "Some servers failed to stop"}
#             )  # Return error response if some servers failed to stop

#     except Exception as e:  # Catch any exceptions during server stopping process
#         print(f"Error stopping servers: {e}")
#         raise HTTPException(
#             status_code=500, detail=f"Error stopping servers: {str(e)}"
#         )  # Raise HTTPException with error details

# Define the scopes for Google API access.
# These scopes define the permissions that the application requests from the user during OAuth.
SCOPES = [
    "https://www.googleapis.com/auth/gmail.send",
    "https://www.googleapis.com/auth/gmail.compose",
    "https://www.googleapis.com/auth/gmail.modify",
    "https://www.googleapis.com/auth/gmail.readonly",
    "https://www.googleapis.com/auth/documents",
    "https://www.googleapis.com/auth/calendar",
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/presentations",
    "https://www.googleapis.com/auth/drive",
    "https://mail.google.com/",
]

# Dictionary to hold the client credentials for OAuth 2.0 flow.
# These credentials should be obtained from the Google Cloud Console for your project.
CREDENTIALS_DICT = {
    "installed": {
        "client_id": os.environ.get("GOOGLE_CLIENT_ID"),  # Your Client ID from Google Cloud Console
        "project_id": os.environ.get("GOOGLE_PROJECT_ID"),  # Your Project ID from Google Cloud Console
        "auth_uri": os.environ.get("GOOGLE_AUTH_URI"),  # OAuth 2.0 Authorize URI from Google Cloud Console
        "token_uri": os.environ.get("GOOGLE_TOKEN_URI"),  # OAuth 2.0 Token URI from Google Cloud Console
        "auth_provider_x509_cert_url": os.environ.get("GOOGLE_AUTH_PROVIDER_x509_CERT_URL"),  # Auth Provider X.509 cert URL from Google Cloud Console
        "client_secret": os.environ.get("GOOGLE_CLIENT_SECRET"),  # Your Client Secret from Google Cloud Console
        "redirect_uris": ["http://localhost"],  # Redirect URIs from Google Cloud Console
    }
}

@app.get("/authenticate-google")
async def authenticate_google():
    """
    Endpoint to authenticate with Google using OAuth 2.0.

    This endpoint checks for existing credentials in 'token.pickle'.
    If credentials are found and valid, it indicates successful authentication.
    If credentials are not found or invalid, it initiates the OAuth 2.0 flow,
    prompting the user to authenticate and authorize the application.
    Upon successful authentication, credentials are saved to 'token.pickle' for future use.

    Returns:
        JSONResponse: Returns a JSON response indicating the success or failure of the authentication process.
                      - On success: {"success": True} with HTTP status code 200.
                      - On failure: {"success": False, "error": str(e)} with HTTP status code 500, including the error message.
    """
    try:
        creds = None  # Initialize credentials variable

        # Check if a token.pickle file exists, which contains previously saved credentials.
        if os.path.exists("../token.pickle"):
            with open("../token.pickle", "rb") as token:
                creds = pickle.load(
                    token
                )  # Load credentials from the token.pickle file
                print(f"Loaded credentials")

        # If there are no (valid) credentials available, let the user log in.
        if not creds or not creds.valid:
            # If credentials are expired but a refresh token is available, refresh the credentials.
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request())
            else:
                # If no valid credentials, create a flow object to perform OAuth.
                flow = InstalledAppFlow.from_client_config(CREDENTIALS_DICT, SCOPES)
                # Run the local server flow to get credentials by prompting user via browser.
                creds = flow.run_local_server(port=0)

            # Save the credentials for the next run
            with open("../token.pickle", "wb") as token:
                pickle.dump(creds, token)

        print(f"Authenticated Google")
        return JSONResponse(
            status_code=200, content={"success": True}
        )  # Return success response

    except Exception as e:
        print(f"Error authenticating Google: {e}")
        return JSONResponse(
            status_code=500, content={"success": False, "error": str(e)}
        )  # Return error response with exception details


# --- Main Application Execution ---
if __name__ == "__main__":
    multiprocessing.freeze_support()  # For Windows executables created with PyInstaller
    uvicorn.run(
        app, host="0.0.0.0", port=5000, reload=False, workers=1
    )  # Run the FastAPI application using Uvicorn server

### scraper/scraper.py ###
import os
import uvicorn
from runnables import *  # Importing runnable classes or functions from runnables.py
from functions import *  # Importing custom functions from functions.py
from helpers import *  # Importing helper functions from helpers.py
from prompts import *  # Importing prompt templates and related utilities from prompts.py
import nest_asyncio  # For running asyncio event loop within another event loop (needed for FastAPI in some environments)
from fastapi import FastAPI  # Importing FastAPI for creating the API application
from pydantic import (
    BaseModel,
)  # Importing BaseModel from Pydantic for request body validation and data modeling
from fastapi.responses import (
    JSONResponse,
)  # Importing JSONResponse for sending JSON responses from API endpoints
from fastapi.middleware.cors import (
    CORSMiddleware,
)  # Importing CORSMiddleware to handle Cross-Origin Resource Sharing
from fastapi import (
    FastAPI,
    HTTPException,
)  # Importing FastAPI and HTTPException for handling HTTP exceptions
from pydantic import BaseModel  # Re-importing BaseModel (likely a typo and redundant)
import os  # Re-importing os (likely a typo and redundant)
from dotenv import load_dotenv

load_dotenv("../.env")  # Load environment variables from .env file

# --- FastAPI Application Initialization ---
app = FastAPI(
    docs_url="/docs", 
    redoc_url=None
    )  # Creating a FastAPI application instance

# --- CORS Middleware Configuration ---
# Configuring CORS to allow requests from any origin.
# This is generally fine for open-source projects or APIs intended for broad use, but be cautious in production environments.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,  # Allows credentials (cookies, authorization headers) to be included in requests
    allow_methods=["*"],  # Allows all HTTP methods (GET, POST, PUT, DELETE, etc.)
    allow_headers=["*"],  # Allows all headers to be included in requests
)


# --- Pydantic Models for Request Body Validation ---
class RedditURL(BaseModel):
    """
    Pydantic model for validating Reddit URL requests.
    Requires a 'url' string field.
    """

    url: str  # Reddit profile URL to scrape


class TwitterURL(BaseModel):
    """
    Pydantic model for validating Twitter URL requests.
    Requires a 'url' string field.
    """

    url: str  # Twitter profile URL to scrape


class LinkedInURL(BaseModel):
    """
    Pydantic model for validating LinkedIn URL requests.
    Requires a 'url' string field.
    """

    url: str  # LinkedIn profile URL to scrape


# --- Global Variables for Runnables ---
# These global variables store initialized Langchain Runnable sequences for Reddit and Twitter.
# Initialized in the `/initiate` endpoint.
reddit_runnable = None  # Runnable for Reddit-related tasks (topic extraction)
twitter_runnable = None  # Runnable for Twitter-related tasks (topic extraction)

# --- Apply nest_asyncio ---
# Applying nest_asyncio to allow asyncio event loops to be nested.
nest_asyncio.apply()


# --- API Endpoints ---
@app.get("/")
async def main():
    """
    Root endpoint of the API.
    Returns a simple welcome message.
    """
    return {
        "message": "Hello, I am Sentient, your private, decentralized and interactive AI companion who feels human"
    }


@app.post("/initiate", status_code=200)
async def initiate():
    """
    Endpoint to initialize the AI model and runnables for Reddit and Twitter.
    This endpoint sets up the global runnable variables required for scraping and topic extraction.
    Returns a success message or an error message if initialization fails.
    """
    global reddit_runnable, twitter_runnable

    # Initialize runnables for Reddit and Twitter using functions from runnables.py
    reddit_runnable = get_reddit_runnable()  # Get runnable for Reddit tasks
    twitter_runnable = get_twitter_runnable()  # Get runnable for Twitter tasks
    try:
        return JSONResponse(
            status_code=200, content={"message": "Model initiated successfully"}
        )  # Return success message
    except Exception as e:
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return error message if exception occurs


@app.post("/scrape-linkedin", status_code=200)
async def scrape_linkedin(profile: LinkedInURL):
    """
    Endpoint to scrape and return LinkedIn profile information.
    Uses the `scrape_linkedin_profile` function to extract structured data from a LinkedIn profile URL.
    Returns the scraped profile data as a JSON response.
    """
    try:
        linkedin_profile = scrape_linkedin_profile(
            profile.url
        )  # Scrape LinkedIn profile data
        return JSONResponse(
            status_code=200, content={"profile": linkedin_profile}
        )  # Return scraped profile data in JSON response
    except Exception as e:
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return error message if exception occurs


@app.post("/scrape-reddit")
async def scrape_reddit(reddit_url: RedditURL):
    """
    Endpoint to extract topics of interest from a Reddit user's profile.
    Leverages the `reddit_scraper` function to fetch subreddit data and then uses a
    `reddit_runnable` (Langchain Runnable) to process this data and extract topics of interest.
    Returns a JSON response containing the extracted topics.
    """
    global reddit_runnable

    try:
        subreddits = reddit_scraper(
            reddit_url.url
        )  # Scrape Reddit user's subreddit data

        response = reddit_runnable.invoke(
            {"subreddits": subreddits}
        )  # Invoke Reddit runnable to extract topics

        try:
            topics = response  # Get topics from the response
            if isinstance(topics, list):
                return JSONResponse(
                    status_code=200, content={"topics": topics}
                )  # Return topics in JSON response
            else:
                raise HTTPException(
                    status_code=500,
                    detail="Invalid response format from the language model.",
                )  # Raise HTTPException for invalid response format
        except Exception as e:
            print(f"Error in scraping-reddit: {str(e)}")
            raise HTTPException(
                status_code=500, detail=f"Error parsing model response: {str(e)}"
            )  # Raise HTTPException for model response parsing error
    except HTTPException as http_exc:
        print(http_exc)
        raise http_exc  # Re-raise HTTPException
    except Exception as e:
        print(f"Error in scraping reddit: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Unexpected error: {str(e)}"
        )  # Raise HTTPException for unexpected errors

@app.post("/scrape-twitter")
async def scrape_twitter(twitter_url: TwitterURL):
    """
    Endpoint to extract topics of interest from a Twitter user's profile.
    Utilizes the `scrape_twitter_data` function to get tweet texts and then employs a
    `twitter_runnable` (Langchain Runnable) to analyze these texts and extract topics of interest.
    Returns a JSON response with the extracted topics.
    """
    global twitter_runnable

    try:
        tweets = scrape_twitter_data(
            twitter_url.url, 20
        )  # Scrape Twitter user's tweet data
        response = twitter_runnable.invoke(
            {"tweets": tweets}
        )  # Invoke Twitter runnable to extract topics
        topics = response  # Get topics from the response
        if isinstance(topics, list):
            return JSONResponse(
                status_code=200, content={"topics": topics}
            )  # Return topics in JSON response
        else:
            raise HTTPException(
                status_code=500,
                detail="Invalid response format from the language model.",
            )  # Raise HTTPException for invalid response format
    except HTTPException as http_exc:
        print(http_exc)
        raise http_exc  # Re-raise HTTPException
    except Exception as e:
        print(f"Error scraping twitter: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Unexpected error: {str(e)}"
        )  # Raise HTTPException for unexpected errors


# --- Main execution block ---
if __name__ == "__main__":
    """
    This block is executed when the script is run directly (not imported as a module).
    It starts the uvicorn server to run the FastAPI application.
    """
    uvicorn.run(app, port=5004)  # Start uvicorn server on port 5004
### scraper/startserv.sh ###
sudo -E <your-venv-path> -m uvicorn scraper:app --host 0.0.0.0 --port 5004
### auth/startserv.sh ###
sudo -E <your-venv-path> -m uvicorn auth:app --host 0.0.0.0 --port 5007
### auth/auth.py ###
from fastapi import FastAPI
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import os
import pickle
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
import uvicorn
from helpers import *
from pydantic import BaseModel
from dotenv import load_dotenv

load_dotenv("../.env") # Load environment variables from .env file

# Initialize FastAPI application
app = FastAPI(
    title="Authentication API",
    description="API for handling Google OAuth2 authentication",
    docs_url="/docs", 
    redoc_url=None
)

# Add CORS middleware to allow cross-origin requests
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins - configure this for production
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods - configure this for production
    allow_headers=["*"],  # Allows all headers - configure this for production
)

# Pydantic model for request body (currently not used in this file, but defined for potential future use)
class BuildCredentialsRequest(BaseModel):
    """
    Pydantic model for build credentials request body.
    Currently not used in this application but can be used in future for dynamic credential building.
    """

    api_name: str
    api_version: str

# Define the scopes for Google API access.
# These scopes define the permissions that the application requests from the user during OAuth.
SCOPES = [
    "https://www.googleapis.com/auth/gmail.send",
    "https://www.googleapis.com/auth/gmail.compose",
    "https://www.googleapis.com/auth/gmail.modify",
    "https://www.googleapis.com/auth/gmail.readonly",
    "https://www.googleapis.com/auth/documents",
    "https://www.googleapis.com/auth/calendar",
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/presentations",
    "https://www.googleapis.com/auth/drive",
    "https://mail.google.com/",
]

# Dictionary to hold the client credentials for OAuth 2.0 flow.
# These credentials should be obtained from the Google Cloud Console for your project.
CREDENTIALS_DICT = {
    "installed": {
        "client_id": os.environ.get("GOOGLE_CLIENT_ID"),  # Your Client ID from Google Cloud Console
        "project_id": os.environ.get("GOOGLE_PROJECT_ID"),  # Your Project ID from Google Cloud Console
        "auth_uri": os.environ.get("GOOGLE_AUTH_URI"),  # OAuth 2.0 Authorize URI from Google Cloud Console
        "token_uri": os.environ.get("GOOGLE_TOKEN_URI"),  # OAuth 2.0 Token URI from Google Cloud Console
        "auth_provider_x509_cert_url": os.environ.get("GOOGLE_AUTH_PROVIDER_x509_CERT_URL"),  # Auth Provider X.509 cert URL from Google Cloud Console
        "client_secret": os.environ.get("GOOGLE_CLIENT_SECRET"),  # Your Client Secret from Google Cloud Console
        "redirect_uris": ["http://localhost"],  # Redirect URIs from Google Cloud Console
    }
}

@app.get("/authenticate-google")
async def authenticate_google():
    """
    Endpoint to authenticate with Google using OAuth 2.0.

    This endpoint checks for existing credentials in 'token.pickle'.
    If credentials are found and valid, it indicates successful authentication.
    If credentials are not found or invalid, it initiates the OAuth 2.0 flow,
    prompting the user to authenticate and authorize the application.
    Upon successful authentication, credentials are saved to 'token.pickle' for future use.

    Returns:
        JSONResponse: Returns a JSON response indicating the success or failure of the authentication process.
                      - On success: {"success": True} with HTTP status code 200.
                      - On failure: {"success": False, "error": str(e)} with HTTP status code 500, including the error message.
    """
    try:
        creds = None  # Initialize credentials variable

        # Check if a token.pickle file exists, which contains previously saved credentials.
        if os.path.exists("../token.pickle"):
            with open("../token.pickle", "rb") as token:
                creds = pickle.load(
                    token
                )  # Load credentials from the token.pickle file
                print(f"Loaded credentials")

        # If there are no (valid) credentials available, let the user log in.
        if not creds or not creds.valid:
            # If credentials are expired but a refresh token is available, refresh the credentials.
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request())
            else:
                # If no valid credentials, create a flow object to perform OAuth.
                flow = InstalledAppFlow.from_client_config(CREDENTIALS_DICT, SCOPES)
                # Run the local server flow to get credentials by prompting user via browser.
                creds = flow.run_local_server(port=0)

            # Save the credentials for the next run
            with open("../token.pickle", "wb") as token:
                pickle.dump(creds, token)

        print(f"Authenticated Google")
        return JSONResponse(
            status_code=200, content={"success": True}
        )  # Return success response

    except Exception as e:
        print(f"Error authenticating Google: {e}")
        return JSONResponse(
            status_code=500, content={"success": False, "error": str(e)}
        )  # Return error response with exception details


if __name__ == "__main__":
    uvicorn.run(app, port=5007)  # Start the FastAPI application using Uvicorn server
### memory/memory.py ###
import os
import uvicorn
import json
from externals import *  # Importing external service integrations or utilities from externals.py
from runnables import *  # Importing runnable classes or functions from runnables.py
from functions import *  # Importing custom functions from functions.py
from constants import *  # Importing constant variables from constants.py
from helpers import *  # Importing helper functions from helpers.py
from prompts import *  # Importing prompt templates and related utilities from prompts.py
import nest_asyncio  # For running asyncio event loop within another event loop (needed for FastAPI in some environments)
from fastapi import FastAPI  # Importing FastAPI for creating the API application
from pydantic import (
    BaseModel,
)  # Importing BaseModel from Pydantic for request body validation and data modeling
from neo4j import (
    GraphDatabase,
)  # Importing GraphDatabase from neo4j for Neo4j interaction
from fastapi.responses import (
    JSONResponse,
    StreamingResponse,
)  # Importing JSONResponse and StreamingResponse for sending JSON responses from API endpoints
from fastapi.middleware.cors import (
    CORSMiddleware,
)  # Importing CORSMiddleware to handle Cross-Origin Resource Sharing
from llama_index.embeddings.huggingface import (
    HuggingFaceEmbedding,
)  # Importing HuggingFaceEmbedding for embedding model from llama_index
from fastapi import FastAPI  # Re-importing FastAPI (likely a typo and redundant)
from pydantic import BaseModel  # Re-importing BaseModel (likely a typo and redundant)
import os  # Re-importing os (likely a typo and redundant)
from neo4j import (
    GraphDatabase,
)  # Re-importing GraphDatabase (likely a typo and redundant)
from dotenv import load_dotenv

load_dotenv("../.env")  # Load environment variables from .env file

# --- FastAPI Application Initialization ---
app = FastAPI(
    docs_url="/docs", 
    redoc_url=None
    )  # Creating a FastAPI application instance

# --- CORS Middleware Configuration ---
# Configuring CORS to allow requests from any origin.
# This is generally fine for open-source projects or APIs intended for broad use, but be cautious in production environments.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,  # Allows credentials (cookies, authorization headers) to be included in requests
    allow_methods=["*"],  # Allows all HTTP methods (GET, POST, PUT, DELETE, etc.)
    allow_headers=["*"],  # Allows all headers to be included in requests
)


# --- Pydantic Models for Request Body Validation ---
class DeleteSubgraphRequest(BaseModel):
    """
    Pydantic model for validating delete subgraph requests.
    Requires a 'source' string field.
    """

    source: str  # Source identifier for the subgraph to be deleted


class Message(BaseModel):
    """
    Pydantic model for validating chat message requests.
    Defines the structure for incoming chat messages.
    """

    original_input: str  # The original user input message
    transformed_input: str  # The transformed/processed user input message
    pricing: str  # Pricing plan of the user (e.g., "free", "pro")
    credits: int  # User's available credits
    chat_id: str  # Identifier for the chat session


class GraphRequest(BaseModel):
    """
    Pydantic model for validating generic graph requests.
    Requires an 'information' string field.
    """

    information: str  # Information string for graph operations


class GraphRAGRequest(BaseModel):
    """
    Pydantic model for validating GraphRAG (Retrieval-Augmented Generation) requests.
    Requires a 'query' string field.
    """

    query: str  # User's query for GraphRAG


# --- Global Variables for Application State ---
# These global variables store initialized models, runnables, database connections, and chat history.
# Initialized in the `/initiate` endpoint.
index = None  # Llama-Index index (not currently used in the provided endpoints)
retriever = None  # Llama-Index retriever (not currently used in the provided endpoints)
embed_model = None  # Embedding model instance (HuggingFaceEmbedding)
chat_history = (
    None  # Stores chat history, likely as a list of messages or a database connection
)
graph_driver = None  # Neo4j graph driver instance for database interaction
chat_runnable = None  # Runnable for handling chat interactions
information_extraction_runnable = (
    None  # Runnable for extracting information (entities, relationships) from text
)
graph_decision_runnable = (
    None  # Runnable for making decisions about graph operations (CRUD)
)
graph_analysis_runnable = None  # Runnable for analyzing graph data
text_dissection_runnable = None  # Runnable for dissecting text into categories
text_conversion_runnable = (
    None  # Runnable for converting structured graph data to unstructured text
)
query_classification_runnable = (
    None  # Runnable for classifying user queries into categories
)
fact_extraction_runnable = None  # Runnable for extracting facts from text
text_summarizer_runnable = None  # Runnable for summarizing text
text_description_runnable = None  # Runnable for generating descriptions for entities

# --- Apply nest_asyncio ---
# Applying nest_asyncio to allow asyncio event loops to be nested.
# This is often needed when running FastAPI applications in environments that may already have an event loop running,
# such as Jupyter notebooks or certain testing frameworks.
nest_asyncio.apply()


# --- API Endpoints ---
@app.get("/")
async def main():
    """
    Root endpoint of the API.
    Returns a simple welcome message.
    """
    return {
        "message": "Hello, I am Sentient, your private, decentralized and interactive AI companion who feels human"
    }


@app.post("/initiate", status_code=200)
async def initiate():
    """
    Endpoint to initialize the AI model, graph driver, and runnables.
    This endpoint sets up the global variables required for other API calls.
    Returns a success message or an error message if initialization fails.
    """
    global embed_model, graph_driver, chat_history, index, information_extraction_runnable, graph_decision_runnable, graph_analysis_runnable, text_dissection_runnable, text_conversion_runnable, query_classification_runnable, fact_extraction_runnable, text_summarizer_runnable, text_description_runnable

    # Initialize HuggingFace embedding model
    embed_model = HuggingFaceEmbedding(model_name=os.environ["EMBEDDING_MODEL_REPO_ID"])

    # Initialize Neo4j graph driver
    graph_driver = GraphDatabase.driver(
        uri=os.environ["NEO4J_URI"],
        auth=(os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"]),
    )

    # Initialize various runnables using functions from runnables.py
    graph_decision_runnable = (
        get_graph_decision_runnable()
    )  # Get runnable for graph decision making (CRUD operations)
    information_extraction_runnable = (
        get_information_extraction_runnable()
    )  # Get runnable for information extraction
    graph_analysis_runnable = (
        get_graph_analysis_runnable()
    )  # Get runnable for graph analysis
    text_dissection_runnable = (
        get_text_dissection_runnable()
    )  # Get runnable for text dissection
    text_conversion_runnable = (
        get_text_conversion_runnable()
    )  # Get runnable for text conversion (graph to text)
    query_classification_runnable = (
        get_query_classification_runnable()
    )  # Get runnable for query classification
    fact_extraction_runnable = (
        get_fact_extraction_runnable()
    )  # Get runnable for fact extraction
    text_summarizer_runnable = (
        get_text_summarizer_runnable()
    )  # Get runnable for text summarization
    text_description_runnable = (
        get_text_description_runnable()
    )  # Get runnable for text description generation

    try:
        return JSONResponse(
            status_code=200, content={"message": "Model initiated successfully"}
        )  # Return success message
    except Exception as e:
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return error message if exception occurs


@app.post("/chat", status_code=200)
async def chat(message: Message):
    """
    Endpoint to handle user chat messages and generate responses.
    This is the main chat endpoint that processes user input, interacts with the knowledge graph,
    performs internet searches if necessary, and streams back the AI assistant's response.
    """
    global index, embed_model, chat_runnable
    global \
        fact_extraction_runnable, \
        text_conversion_runnable, \
        information_extraction_runnable
    global \
        graph_analysis_runnable, \
        graph_decision_runnable, \
        query_classification_runnable, \
        text_description_runnable
        
    print("Query classification runnable", query_classification_runnable)

    try:
        with open("../../userProfileDb.json", "r", encoding="utf-8") as f:
            db = json.load(f)  # Load user profile database

        chat_history = get_chat_history()  # Retrieve chat history for the given chat_id

        chat_runnable = get_chat_runnable(
            chat_history
        )  # Initialize chat runnable with chat history

        username = db["userData"]["personalInfo"][
            "name"
        ]  # Get username from user profile

        transformed_input = (
            message.transformed_input
        )  # Get transformed user input (pre-processed query)

        pricing_plan = message.pricing  # Get user's pricing plan
        credits = message.credits  # Get user's available credits

        async def response_generator():
            """
            Async generator function to stream the AI assistant's response.
            Handles memory updates, internet search, and response generation based on user's pricing plan and credits.
            """
            memory_used = (
                False  # Flag to track if memory (knowledge graph) was used for updates
            )
            agents_used = (
                False  # Flag to track if agents (not used in this endpoint) were used
            )
            internet_used = False  # Flag to track if internet search was performed
            user_context = None  # Context retrieved from user's knowledge graph
            internet_context = None  # Context retrieved from internet search
            pro_used = False  # Flag to track if pro features were used (memory updates)
            note = ""  # Note to append to the response, e.g., for credit expiry

            yield (
                json.dumps(
                    {
                        "type": "userMessage",
                        "message": message.original_input,
                        "memoryUsed": False,
                        "agentsUsed": False,
                        "internetUsed": False,
                    }
                )
                + "\n"
            )  # Yield user's message
            await asyncio.sleep(0.05)  # Small delay for UI responsiveness

            if pricing_plan == "free":
                if credits <= 0:
                    # Free plan and no credits left: retrieval from knowledge graph only, no memory updates
                    yield (
                        json.dumps(
                            {
                                "type": "intermediary",
                                "message": "Retrieving memories...",
                            }
                        )
                        + "\n"
                    )  # Yield intermediary message
                    await asyncio.sleep(0.05)  # Small delay for UI responsiveness

                    user_context = query_user_profile(
                        transformed_input,
                        graph_driver,
                        embed_model,
                        text_conversion_runnable,
                        query_classification_runnable,
                    )  # Retrieve context from knowledge graph
                    note = "Sorry friend, could have updated my memory for this query. But, that is a pro feature and your daily credits have expired. You can always upgrade to pro from the settings page"  # Set note about credit expiry

                else:
                    # Free plan with credits: memory update and retrieval, internet search if classified as needed
                    yield (
                        json.dumps(
                            {"type": "intermediary", "message": "Updating memories..."}
                        )
                        + "\n"
                    )  # Yield intermediary message
                    await asyncio.sleep(0.05)  # Small delay for UI responsiveness

                    memory_used = True  # Mark memory as used
                    pro_used = True  # Mark pro features as used
                    points = fact_extraction_runnable.invoke(
                        {"paragraph": transformed_input, "username": username}
                    )  # Extract facts from user input

                    for point in points:
                        crud_graph_operations(
                            point,
                            graph_driver,
                            embed_model,
                            query_classification_runnable,
                            information_extraction_runnable,
                            graph_analysis_runnable,
                            graph_decision_runnable,
                            text_description_runnable,
                        )  # Perform CRUD operations to update graph

                    yield (
                        json.dumps(
                            {
                                "type": "intermediary",
                                "message": "Retrieving memories...",
                            }
                        )
                        + "\n"
                    )  # Yield intermediary message
                    await asyncio.sleep(0.05)  # Small delay for UI responsiveness

                    user_context = query_user_profile(
                        transformed_input,
                        graph_driver,
                        embed_model,
                        text_conversion_runnable,
                        query_classification_runnable,
                    )  # Retrieve context from knowledge graph

                    internet_classification = await classify_context(
                        transformed_input, "internet"
                    )  # Classify if internet search is needed

                    if internet_classification == "Internet":
                        yield (
                            json.dumps(
                                {
                                    "type": "intermediary",
                                    "message": "Searching the internet...",
                                }
                            )
                            + "\n"
                        )  # Yield intermediary message
                        internet_context = await perform_internet_search(
                            transformed_input
                        )  # Perform internet search
                        internet_used = True  # Mark internet as used
                    else:
                        internet_context = None  # No internet context needed
            else:
                # Pro plan: memory update and retrieval, internet search if classified as needed
                yield (
                    json.dumps(
                        {"type": "intermediary", "message": "Updating memories..."}
                    )
                    + "\n"
                )  # Yield intermediary message
                await asyncio.sleep(0.05)  # Small delay for UI responsiveness

                memory_used = True  # Mark memory as used
                pro_used = True  # Mark pro features as used
                points = fact_extraction_runnable.invoke(
                    {"paragraph": transformed_input, "username": username}
                )  # Extract facts from user input

                for point in points:
                    crud_graph_operations(
                        point,
                        graph_driver,
                        embed_model,
                        query_classification_runnable,
                        information_extraction_runnable,
                        graph_analysis_runnable,
                        graph_decision_runnable,
                        text_description_runnable,
                    )  # Perform CRUD operations to update graph

                yield (
                    json.dumps(
                        {"type": "intermediary", "message": "Retrieving memories..."}
                    )
                    + "\n"
                )  # Yield intermediary message
                await asyncio.sleep(0.05)  # Small delay for UI responsiveness

                user_context = query_user_profile(
                    transformed_input,
                    graph_driver,
                    embed_model,
                    text_conversion_runnable,
                    query_classification_runnable,
                )  # Retrieve context from knowledge graph

                internet_classification = await classify_context(
                    transformed_input, "internet"
                )  # Classify if internet search is needed

                if internet_classification == "Internet":
                    yield (
                        json.dumps(
                            {
                                "type": "intermediary",
                                "message": "Searching the internet...",
                            }
                        )
                        + "\n"
                    )  # Yield intermediary message
                    internet_context = await perform_internet_search(
                        transformed_input
                    )  # Perform internet search
                    internet_used = True  # Mark internet as used
                else:
                    internet_context = None  # No internet context needed

            with open("../../userProfileDb.json", "r", encoding="utf-8") as f:
                db = json.load(
                    f
                )  # Load user profile database again (redundant load, consider optimizing)

            personality_description = db["userData"].get(
                "personality", "None"
            )  # Get personality description from user profile

            # Stream response from chat runnable
            async for token in generate_streaming_response(
                chat_runnable,
                inputs={
                    "query": transformed_input,
                    "user_context": user_context,
                    "internet_context": internet_context,
                    "name": username,
                    "personality": personality_description,
                },
                stream=True,  # Enable streaming response
            ):
                if isinstance(token, str):
                    yield (
                        json.dumps(
                            {"type": "assistantStream", "token": token, "done": False}
                        )
                        + "\n"
                    )  # Yield assistant's stream token
                else:
                    yield (
                        json.dumps(
                            {
                                "type": "assistantStream",
                                "token": "\n\n"
                                + note,  # Append note to the final token
                                "done": True,
                                "memoryUsed": memory_used,
                                "agentsUsed": agents_used,
                                "internetUsed": internet_used,
                                "proUsed": pro_used,
                            }
                        )
                        + "\n"
                    )  # Yield final token with flags and note
                await asyncio.sleep(0.05)  # Small delay for UI responsiveness
            await asyncio.sleep(0.05)  # Small delay after response generation

        return StreamingResponse(
            response_generator(), media_type="application/json"
        )  # Return streaming response

    except Exception as e:
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return error message if exception occurs


@app.post("/graphrag", status_code=200)
async def graphrag(request: GraphRAGRequest):
    """
    Endpoint to process a user profile query using GraphRAG (Retrieval-Augmented Generation).

    This endpoint takes a user query, retrieves relevant context from the knowledge graph using GraphRAG,
    and returns the context as a JSON response. This is useful for directly querying the knowledge graph
    without engaging in a full chat conversation.
    """
    global \
        graph_driver, \
        embed_model, \
        text_conversion_runnable, \
        query_classification_runnable

    try:
        context = query_user_profile(
            request.query,
            graph_driver,
            embed_model,
            text_conversion_runnable,
            query_classification_runnable,
        )  # Query user profile using GraphRAG
        return JSONResponse(
            status_code=200, content={"context": context}
        )  # Return context in JSON response
    except Exception as e:
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return error message if exception occurs


@app.post("/create-graph", status_code=200)
async def create_graph():
    """
    Endpoint to create a knowledge graph from documents in the input directory.

    This endpoint initiates the process of building a knowledge graph by processing text documents
    found in the "../input" directory. It clears any existing graph, then dissects, extracts, and loads
    information from these documents into the Neo4j graph database.
    """
    global \
        graph_driver, \
        embed_model, \
        text_dissection_runnable, \
        information_extraction_runnable, \
        text_summarizer_runnable

    try:
        input_dir = "../input"  # Define input directory for documents

        with open("../../userProfileDb.json", "r", encoding="utf-8") as f:
            db = json.load(f)  # Load user profile database to get username

        username = db["userData"]["personalInfo"].get(
            "name", "User"
        )  # Get username from user profile

        extracted_texts = []  # Initialize list to store extracted texts
        for file_name in os.listdir(
            input_dir
        ):  # Iterate through files in the input directory
            file_path = os.path.join(input_dir, file_name)  # Construct file path
            if os.path.isfile(file_path):  # Check if it's a file
                with open(file_path, "r", encoding="utf-8") as file:
                    text_content = (
                        file.read().strip()
                    )  # Read file content and strip whitespace
                    if text_content:
                        extracted_texts.append(
                            {"text": text_content, "source": file_name}
                        )  # Append text and source to list

        if not extracted_texts:
            return JSONResponse(
                status_code=400,
                content={
                    "message": "No content found in input documents to create the graph."
                },  # Return error if no content found
            )

        def clear_graph():
            """Clears all nodes and relationships from the Neo4j graph database."""
            with graph_driver.session() as session:  # Open Neo4j session
                session.run(
                    "MATCH (n) DETACH DELETE n"
                )  # Cypher query to delete all nodes and relationships
                print(
                    "All nodes and relationships deleted successfully."
                )  # Print confirmation message

        clear_graph()  # Clear existing graph before creating a new one

        build_initial_knowledge_graph(
            username,
            extracted_texts,
            graph_driver,
            embed_model,
            text_dissection_runnable,
            information_extraction_runnable,
        )  # Build initial knowledge graph from extracted texts

        return JSONResponse(
            status_code=200, content={"message": "Graph created successfully."}
        )  # Return success message

    except FileNotFoundError as e:
        print(e)
        return JSONResponse(
            status_code=404,
            content={"message": "Input directory or documents not found."},
        )  # Return error if input directory not found
    except Exception as e:
        print(f"Error creating graph: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "message": "An error occurred while creating the graph.",
                "error": str(e),
            },  # Return general error message
        )


@app.post("/delete-subgraph", status_code=200)
async def delete_subgraph(request: DeleteSubgraphRequest):
    """
    Endpoint to delete a subgraph from the knowledge graph based on a source name.

    This endpoint allows for the removal of specific parts of the knowledge graph that are associated
    with a given data source (e.g., LinkedIn profile, Reddit data). It identifies nodes by their 'source'
    property and deletes them along with their relationships.
    """
    global graph_driver

    try:
        with open("../../userProfileDb.json", "r", encoding="utf-8") as f:
            db = json.load(f)  # Load user profile database to get username

        username = (
            db["userData"]["personalInfo"].get("name", "User").lower()
        )  # Get lowercase username from user profile

        source_name = request.source  # Get source name from request

        if not source_name:
            return JSONResponse(
                status_code=400,
                content={
                    "message": "Missing source_name parameter."
                },  # Return error if source_name is missing
            )

        SOURCES = {
            "linkedin": f"{username}_linkedin_profile.txt",
            "reddit": f"{username}_reddit_profile.txt",
            "twitter": f"{username}_twitter_profile.txt",
        }  # Mapping of source names to file names

        file_name = SOURCES[source_name]  # Get file name corresponding to source name
        if not file_name:
            return JSONResponse(
                status_code=400,
                content={
                    "message": f"No file mapping found for source name: {source_name}"
                },  # Return error if no file mapping found
            )

        delete_source_subgraph(
            graph_driver, file_name
        )  # Delete subgraph from Neo4j based on file name

        os.remove(f"../input/{file_name}")  # Remove the corresponding input file

        return JSONResponse(
            status_code=200,
            content={
                "message": f"Subgraph related to {file_name} deleted successfully."
            },  # Return success message
        )

    except Exception as e:
        print(f"Error deleting subgraph: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "message": "An error occurred while deleting the subgraph.",
                "error": str(e),
            },  # Return general error message
        )


@app.post("/create-document", status_code=200)
async def create_document():
    """
    Endpoint to create and summarize personality documents based on user profile data.

    This endpoint generates text documents representing different aspects of the user's personality
    and profile (e.g., personality traits, LinkedIn profile summary, Reddit interests, Twitter interests).
    It uses a text summarizer runnable to condense the information and saves these documents to the "../input" directory.
    """
    global text_summarizer_runnable

    try:
        with open("../../userProfileDb.json", "r", encoding="utf-8") as f:
            db = json.load(f)  # Load user profile database

        username = db["userData"]["personalInfo"].get(
            "name", "User"
        )  # Get username from user profile
        personality_type = db["userData"].get(
            "personalityType", ""
        )  # Get personality type from user profile
        structured_linkedin_profile = db["userData"].get(
            "linkedInProfile", {}
        )  # Get LinkedIn profile data
        reddit_profile = db["userData"].get(
            "redditProfile", []
        )  # Get Reddit profile data (interests)
        twitter_profile = db["userData"].get(
            "twitterProfile", []
        )  # Get Twitter profile data (interests)
        input_dir = "../input"  # Define input directory

        os.makedirs(input_dir, exist_ok=True)  # Ensure input directory exists

        for file in os.listdir(
            input_dir
        ):  # Clear existing files in the input directory
            file_path = os.path.join(input_dir, file)
            if os.path.isfile(file_path):
                os.remove(file_path)

        trait_descriptions = []  # Initialize list to store personality trait descriptions

        for trait in personality_type:  # Iterate through each personality trait
            if trait in PERSONALITY_DESCRIPTIONS:
                description = f"{trait}: {PERSONALITY_DESCRIPTIONS[trait]}"  # Get description for the trait
                trait_descriptions.append(description)  # Append description to list
                filename = f"{username.lower()}_{trait.lower()}.txt"  # Construct filename for trait document
                filepath = os.path.join(input_dir, filename)  # Construct file path
                summarized_paragraph = text_summarizer_runnable.invoke(
                    {"user_name": username, "text": description}
                )  # Summarize trait description

                with open(filepath, "w", encoding="utf-8") as file:
                    file.write(
                        summarized_paragraph
                    )  # Write summarized description to file

        unified_personality_description = (
            f"{username}'s Personality:\n\n"
            + "\n".join(
                trait_descriptions
            )  # Create unified personality description string
        )

        if structured_linkedin_profile:
            linkedin_profile_file = os.path.join(
                input_dir,
                f"{username.lower()}_linkedin_profile.txt",  # Construct filename for LinkedIn profile document
            )
            summarized_paragraph = text_summarizer_runnable.invoke(
                {"user_name": username, "text": structured_linkedin_profile}
            )  # Summarize LinkedIn profile

            with open(linkedin_profile_file, "w", encoding="utf-8") as file:
                file.write(
                    summarized_paragraph
                )  # Write summarized LinkedIn profile to file

        if reddit_profile:
            reddit_profile_file = os.path.join(
                input_dir,
                f"{username.lower()}_reddit_profile.txt",  # Construct filename for Reddit profile document
            )
            summarized_paragraph = text_summarizer_runnable.invoke(
                {
                    "user_name": username,
                    "text": "Interests: " + (",").join(reddit_profile),
                }
            )  # Summarize Reddit interests

            with open(reddit_profile_file, "w", encoding="utf-8") as file:
                file.write(
                    summarized_paragraph
                )  # Write summarized Reddit interests to file

        if twitter_profile:
            twitter_profile_file = os.path.join(
                input_dir,
                f"{username.lower()}_twitter_profile.txt",  # Construct filename for Twitter profile document
            )
            summarized_paragraph = text_summarizer_runnable.invoke(
                {
                    "user_name": username,
                    "text": "Interests: " + (",").join(twitter_profile),
                }
            )  # Summarize Twitter interests

            with open(twitter_profile_file, "w", encoding="utf-8") as file:
                file.write(
                    summarized_paragraph
                )  # Write summarized Twitter interests to file

        return JSONResponse(
            status_code=200,
            content={
                "message": "Documents created and personality saved successfully",
                "personality": unified_personality_description,  # Return unified personality description in response
            },
        )

    except Exception as e:
        return JSONResponse(
            status_code=500, content={"message": str(e)}
        )  # Return error message if exception occurs


@app.post("/customize-graph", status_code=200)
async def customize_graph(request: GraphRequest):
    """
    Endpoint to customize the knowledge graph with new information provided in the request.

    This endpoint takes unstructured text information from the request, extracts facts from it,
    and performs CRUD operations on the knowledge graph to incorporate this new information.
    This allows users to interactively add or modify information in their personal knowledge graph.
    """
    global \
        information_extraction_runnable, \
        graph_decision_runnable, \
        graph_driver, \
        embed_model, \
        query_classification_runnable, \
        graph_analysis_runnable, \
        text_description_runnable, \
        fact_extraction_runnable
    with open("../../userProfileDb.json", "r", encoding="utf-8") as f:
        db = json.load(f)  # Load user profile database to get username

    username = db["userData"]["personalInfo"]["name"]  # Get username from user profile

    try:
        points = fact_extraction_runnable.invoke(
            {"paragraph": request.information, "username": username}
        )  # Extract facts from the provided information

        for point in points:
            crud_graph_operations(
                point,
                graph_driver,
                embed_model,
                query_classification_runnable,
                information_extraction_runnable,
                graph_analysis_runnable,
                graph_decision_runnable,
                text_description_runnable,
            )  # Perform CRUD operations to customize graph

        return JSONResponse(
            status_code=200,
            content={
                "message": "Graph customized successfully."
            },  # Return success message
        )

    except Exception as e:
        print(e)
        return JSONResponse(
            status_code=500,
            content={
                "message": f"An error occurred: {str(e)}"
            },  # Return error message if exception occurs
        )


# --- Main execution block ---
if __name__ == "__main__":
    """
    This block is executed when the script is run directly (not imported as a module).
    It starts the uvicorn server to run the FastAPI application.
    """
    uvicorn.run(app, port=5002)  # Start uvicorn server on port 5002

### memory/startserv.sh ###
sudo -E <your-venv-path> -m uvicorn memory:app --host 0.0.0.0 --port 5002
