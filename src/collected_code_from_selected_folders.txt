### model\chat\chat.py ###
import os
import uvicorn
import json
import asyncio
from runnables import *
from functions import *
from externals import *
from helpers import *
from prompts import *
import nest_asyncio
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from typing import Dict, Any, AsyncGenerator
from dotenv import load_dotenv
import time

load_dotenv("../.env")  # Load environment variables from .env file

# --- FastAPI Application ---
app = FastAPI(
    title="Chat API", description="API for chat functionalities",
    docs_url="/docs",
    redoc_url=None
)

# --- CORS Middleware ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models for Request Bodies ---
class Message(BaseModel):
    """
    Pydantic model for the chat message request body.
    """
    original_input: str
    transformed_input: str
    pricing: str
    credits: int
    chat_id: str

# --- Global Variables and Database Setup ---
db_path = os.path.join(os.path.dirname(__file__), "..", "..", "chatsDb.json")
db_lock = asyncio.Lock()  # Lock for synchronizing database access

def load_db():
    """Load the database from chatsDb.json, initializing with {"messages": []} if it doesn't exist or is invalid."""
    try:
        with open(db_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        print ("DB NOT FOUND!")
        return {"messages": []}

def save_db(data):
    """Save the data to chatsDb.json."""
    with open(db_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4)

db_data = load_db()  # Load database into memory at startup
chat_runnable = None  # Global chat runnable, initialized later

# --- Apply nest_asyncio ---
nest_asyncio.apply()

# --- API Endpoints ---
@app.get("/", status_code=200)
async def main() -> Dict[str, str]:
    """Root endpoint of the Chat API."""
    return {
        "message": "Hello, I am Sentient, your private, decentralized and interactive AI companion who feels human"
    }

@app.post("/initiate", status_code=200)
async def initiate() -> JSONResponse:
    """Endpoint to initiate the Chat API model."""
    try:
        return JSONResponse(
            status_code=200, content={"message": "Model initiated successfully"}
        )
    except Exception as e:
        print(f"Error initiating chat: {e}")
        return JSONResponse(status_code=500, content={"message": str(e)})

@app.get("/get-chat-history", status_code=200)
async def get_chat_history():
    """Retrieve the chat history."""
    async with db_lock:
        return JSONResponse(status_code=200, content={"messages": db_data["messages"]})

@app.post("/clear-chat-history", status_code=200)
async def clear_chat_history():
    """Clear the chat history."""
    async with db_lock:
        db_data["messages"] = []
        save_db(db_data)
    return JSONResponse(status_code=200, content={"message": "Chat history cleared"})

@app.post("/chat", status_code=200)
async def chat(message: Message):
    """Handle chat interactions with streaming responses."""
    global chat_runnable
    try:
        with open("../../userProfileDb.json", "r", encoding="utf-8") as f:
            user_db = json.load(f)

        if not chat_runnable:
            chat_runnable = get_chat_runnable(db_data["messages"])

        username = user_db["userData"]["personalInfo"]["name"]
        transformed_input = message.transformed_input
        pricing_plan = message.pricing
        credits = message.credits

        async def response_generator():
            memory_used = False
            agents_used = False
            internet_used = False
            user_context = None
            internet_context = None
            pro_used = False
            note = ""

            # Save user message
            user_msg = {
                "id": str(int(time.time() * 1000)),
                "message": message.original_input,
                "isUser": True,
                "memoryUsed": False,
                "agentsUsed": False,
                "internetUsed": False
            }
            async with db_lock:
                db_data["messages"].append(user_msg)
                save_db(db_data)

            yield json.dumps({
                "type": "userMessage",
                "message": message.original_input,
                "memoryUsed": False,
                "agentsUsed": False,
                "internetUsed": False
            }) + "\n"
            await asyncio.sleep(0.05)

            yield json.dumps({"type": "intermediary", "message": "Processing chat response..."}) + "\n"
            await asyncio.sleep(0.05)

            context_classification = await classify_context(transformed_input, "category")
            if "personal" in context_classification["class"]:
                yield json.dumps({"type": "intermediary", "message": "Retrieving memories..."}) + "\n"
                memory_used = True
                user_context = await perform_graphrag(transformed_input)
                
            note = ""
            internet_classification = await classify_context(transformed_input, "internet")
            if pricing_plan == "free" and internet_classification["class"] == "Internet" and credits > 0:
                yield json.dumps({"type": "intermediary", "message": "Searching the internet..."}) + "\n"
                internet_context = await perform_internet_search(transformed_input)
                internet_used = True
                pro_used = True
            elif pricing_plan != "free" and internet_classification["class"] == "Internet":
                yield json.dumps({"type": "intermediary", "message": "Searching the internet..."}) + "\n"
                internet_context = await perform_internet_search(transformed_input)
                internet_used = True
                pro_used = True
            else:
                note = "Sorry, internet search is a pro feature and requires credits on the free plan."

            personality = user_db["userData"].get("personality", "None")
            assistant_msg = {
                "id": str(int(time.time() * 1000)),
                "message": "",
                "isUser": False,
                "memoryUsed": memory_used,
                "agentsUsed": agents_used,
                "internetUsed": internet_used
            }
            async with db_lock:
                db_data["messages"].append(assistant_msg)
                save_db(db_data)

            async for token in generate_streaming_response(
                chat_runnable,
                inputs={
                    "query": transformed_input,
                    "user_context": user_context,
                    "internet_context": internet_context,
                    "name": username,
                    "personality": personality
                },
                stream=True
            ):
                if isinstance(token, str):
                    assistant_msg["message"] += token
                    async with db_lock:
                        save_db(db_data)
                    yield json.dumps({
                        "type": "assistantStream",
                        "token": token,
                        "done": False,
                        "messageId": assistant_msg["id"]
                    }) + "\n"
                    await asyncio.sleep(0.05)
                else:
                    assistant_msg["message"] += "\n\n" + note
                    async with db_lock:
                        save_db(db_data)
                    yield json.dumps({
                        "type": "assistantStream",
                        "token": "\n\n" + note,
                        "done": True,
                        "memoryUsed": memory_used,
                        "agentsUsed": agents_used,
                        "internetUsed": internet_used,
                        "proUsed": pro_used,
                        "messageId": assistant_msg["id"]
                    }) + "\n"

        return StreamingResponse(response_generator(), media_type="application/json")
    except Exception as e:
        print(f"Error executing chat: {e}")
        return JSONResponse(status_code=500, content={"message": str(e)})

if __name__ == "__main__":
    uvicorn.run(app, port=5003)
### model\chat\externals.py ###
import httpx
import os
from helpers import *
from typing import Dict, Any, Optional
from dotenv import load_dotenv

load_dotenv("../.env")  # Load environment variables from .env file

async def classify_context(query: str, context: str) -> Dict[str, Any]:
    """
    Asynchronously calls the /context-classify endpoint to classify a query.

    This function sends a POST request to the context classification service
    to determine the context of the given query.

    Args:
        query (str): The query string to be classified.
        context (str): The context in which the query is being made (e.g., "category", "internet").

    Returns:
        Dict[str, Any]: A dictionary containing the classification result.
                         If successful, it returns a dictionary with the 'classification' key.
                         If an error occurs, it returns a dictionary with an 'error' key
                         containing the error message.
    """
    try:
        port: Optional[str] = os.environ.get(
            "APP_SERVER_PORT"
        )  # Get the port from environment variables
        async with httpx.AsyncClient(
            timeout=None
        ) as client:  # Create an async HTTP client with no timeout
            response = await client.post(
                f"http://localhost:{port}/context-classify",
                json={"query": query, "context": context},
            )  # Send POST request to classify context

        if response.status_code == 200:  # Check if the request was successful
            return response.json()[
                "classification"
            ]  # Return the classification from the response
        else:
            return {
                "error": response.text
            }  # Return an error dictionary with the response text

    except Exception as e:  # Catch any exceptions during the process
        print(f"Error calling classify_context: {e}")
        return {
            "error": f"Error calling context-classify: {str(e)}"
        }  # Return an error dictionary with the exception message


async def perform_internet_search(query: str) -> Dict[str, Any]:
    """
    Asynchronously calls the /internet-search endpoint to fetch search results and summarize them.

    This function sends a POST request to the internet search service to
    retrieve and summarize internet search results for the given query.

    Args:
        query (str): The query string to be used for internet search.

    Returns:
        Dict[str, Any]: A dictionary containing the internet search context.
                         If successful, it returns a dictionary with the 'internet_context' key.
                         If an error occurs, it returns a dictionary with an 'error' key
                         containing the error message.
    """
    try:
        port: Optional[str] = os.environ.get(
            "APP_SERVER_PORT"
        )  # Get the port from environment variables
        async with httpx.AsyncClient(
            timeout=None
        ) as client:  # Create an async HTTP client with no timeout
            response = await client.post(
                f"http://localhost:{port}/internet-search", json={"query": query}
            )  # Send POST request for internet search

        if response.status_code == 200:  # Check if the request was successful
            return response.json()[
                "internet_context"
            ]  # Return the internet context from the response
        else:
            return {
                "error": response.text
            }  # Return an error dictionary with the response text

    except Exception as e:  # Catch any exceptions during the process
        print(f"Error performing internet search: {e}")
        return {
            "error": f"Error calling internet-search: {str(e)}"
        }  # Return an error dictionary with the exception message


async def perform_graphrag(query: str) -> Dict[str, Any]:
    """
    Asynchronously calls the /graphrag endpoint to query the user profile and get graphrag context.

    This function sends a POST request to the graphrag service to query the user profile
    and retrieve relevant context using graph-based retrieval-augmented generation (RAG).

    Args:
        query (str): The query string to be used for graph-based RAG.

    Returns:
        Dict[str, Any]: A dictionary containing the graphrag context.
                         If successful, it returns a dictionary with the 'context' key.
                         If an error occurs, it returns a dictionary with an 'error' key
                         containing the error message.
    """
    try:
        port: Optional[str] = os.environ.get(
            "APP_SERVER_PORT"
        )  # Get the port from environment variables
        async with httpx.AsyncClient(
            timeout=None
        ) as client:  # Create an async HTTP client with no timeout
            response = await client.post(
                f"http://localhost:{port}/graphrag", json={"query": query}
            )  # Send POST request for graphrag

        if response.status_code == 200:  # Check if the request was successful
            return response.json()["context"]  # Return the context from the response
        else:
            return {
                "error": response.text
            }  # Return an error dictionary with the response text

    except Exception as e:  # Catch any exceptions during the process
        print(f"Error performing graphrag: {e}")
        return {
            "error": f"Error calling graphrag: {str(e)}"
        }  # Return an error dictionary with the exception message

### model\chat\functions.py ###
import os
from prompts import *
from wrapt_timeout_decorator import *
from helpers import *
import json
import requests
import asyncio
from typing import Dict, Any, List, Optional, AsyncGenerator
from dotenv import load_dotenv

load_dotenv("../.env")  # Load environment variables from .env file

async def generate_streaming_response(
    runnable, inputs: Dict[str, Any], stream: bool = False
) -> AsyncGenerator[Any, None]:
    """
    Generic function to generate a streaming response from any runnable.

    This function abstracts the process of invoking a runnable and handling its response,
    whether it's a standard response or a streaming one. It checks if the runnable supports
    streaming and calls the appropriate method.

    Args:
        runnable: The runnable object (e.g., chain, agent runnable) to invoke.
        inputs (Dict[str, Any]): Input dictionary for the runnable.
        stream (bool): If True, attempt to generate a streaming response if supported by the runnable (default is False).

    Yields:
        AsyncGenerator[Any, None]: Asynchronously yields tokens or the full response depending on streaming.
                                  Yields None if an error occurs during response generation.
    """
    try:
        if stream and hasattr(
            runnable, "stream_response"
        ):  # Check if streaming is enabled and runnable supports it
            for token in await asyncio.to_thread(lambda: runnable.stream_response(inputs)):
                yield token  # Yield each token from the streaming response
        else:  # Handle non-streaming response
            response = runnable.invoke(
                inputs
            )  # Invoke the runnable to get a standard response
            yield response  # Yield the complete response

    except Exception as e:  # Catch any exceptions during response generation
        print(f"An error occurred: {e}")
        yield None  # Yield None to indicate an error occurred


def generate_response(
    runnable,
    message: str,
    user_context: Optional[str],
    internet_context: Optional[str],
    username: str,
) -> Optional[Dict[str, Any]]:
    """
    Generate a response using the provided runnable, incorporating user and internet context.

    This function retrieves user personality description from a user profile database,
    and invokes the given runnable with the message, context information, username, and personality.

    Args:
        runnable: The runnable object (e.g., agent runnable) to use for response generation.
        message (str): The user's input message.
        user_context (Optional[str]): Context retrieved from user's personal memory or graph (optional).
        internet_context (Optional[str]): Context retrieved from internet search (optional).
        username (str): The username of the user.

    Returns:
        Optional[Dict[str, Any]]: The response generated by the runnable, or None if an error occurs.
    """
    try:
        with open(
            "../../userProfileDb.json", "r", encoding="utf-8"
        ) as f:  # Open and load user profile database
            db = json.load(f)  # Load user profile data from JSON file

        personality_description: str = db["userData"].get(
            "personality", "None"
        )  # Extract personality description from database, default to "None"

        response = runnable.invoke(
            {
                "query": message,
                "user_context": user_context,
                "internet_context": internet_context,
                "name": username,
                "personality": personality_description,
            }
        )  # Invoke runnable with all inputs

        return response  # Return the generated response
    except Exception as e:  # Catch any exceptions during response generation
        print(f"An error occurred in generating response: {e}")
        return None  # Return None to indicate an error occurred


def get_reframed_internet_query(internet_query_reframe_runnable, input: str) -> str:
    """
    Reframes the internet query using the provided runnable.

    This function takes a user input and uses a runnable, specifically designed
    for reframing internet queries, to generate a more effective search query.

    Args:
        internet_query_reframe_runnable: The runnable object designed for reframing internet queries.
        input (str): The original user input to be reframed into an internet search query.

    Returns:
        str: The reframed internet search query.
    """
    reframed_query: str = internet_query_reframe_runnable.invoke(
        {"query": input}
    )  # Invoke the query reframe runnable
    return reframed_query  # Return the reframed query


def get_search_results(reframed_query: str) -> List[Dict[str, Optional[str]]]:
    """
    Fetch and clean descriptions from a web search API based on the provided query.

    This function uses the Brave Search API to fetch web search results for a given query.
    It extracts titles, URLs, and descriptions from the API response and cleans the descriptions
    to remove HTML tags and unescape HTML entities.

    Args:
        reframed_query (str): The search query string to be used for fetching web search results.

    Returns:
        List[Dict[str, Optional[str]]]: A list of dictionaries, each containing the 'title', 'url', and 'description'
                                         of a search result. Returns an empty list if there's an error or no results.
    """
    try:
        params: Dict[str, str] = {  # Parameters for the Brave Search API request
            "q": reframed_query,  # The search query
        }

        headers: Dict[str, str] = {  # Headers for the Brave Search API request
            "Accept": "application/json",
            "Accept-Encoding": "gzip",
            "X-Subscription-Token": os.getenv(
                "BRAVE_SUBSCRIPTION_TOKEN"
            ),  # API token for Brave Search
        }

        response: requests.Response = requests.get(
            os.getenv("BRAVE_BASE_URL"), headers=headers, params=params
        )  # Send GET request to Brave Search API

        if response.status_code == 200:  # Check if the API request was successful
            results = response.json()  # Parse JSON response

            descriptions: List[
                Dict[str, Optional[str]]
            ] = []  # Initialize list to store descriptions
            for item in results.get("web", {}).get("results", [])[
                :5
            ]:  # Iterate through the top 5 web search results
                descriptions.append(
                    {  # Append extracted and raw data to descriptions list
                        "title": item.get("title"),
                        "url": item.get("url"),
                        "description": item.get("description"),
                    }
                )

            clean_descriptions: List[
                Dict[str, Optional[str]]
            ] = [  # Clean descriptions to remove html tags and unescape html characters
                {
                    "title": entry["title"],
                    "url": entry["url"],
                    "description": clean_description(
                        entry["description"]
                    ),  # Clean the description text
                }
                for entry in descriptions  # Iterate over descriptions to clean each description
            ]

            return clean_descriptions  # Return the list of cleaned descriptions

        else:  # Handle non-200 status codes
            raise Exception(
                f"API request failed with status code {response.status_code}: {response.text}"
            )  # Raise exception with error details

    except Exception as e:  # Catch any exceptions during search or processing
        print(f"Error fetching or processing descriptions: {e}")
        return []  # Return empty list in case of error


def get_search_summary(
    internet_summary_runnable, search_results: List[Dict[str, Optional[str]]]
) -> Optional[Dict[str, Any]]:
    """
    Summarize internet search results using the provided runnable.

    This function takes a list of search results and uses a runnable, specifically designed
    for summarizing internet search results, to generate a concise summary.

    Args:
        internet_summary_runnable: The runnable object designed for summarizing internet search results.
        search_results (List[Dict[str, Optional[str]]]): A list of dictionaries, each containing search result details.

    Returns:
        Optional[Dict[str, Any]]: The summary of the search results generated by the runnable,
                                  or None if an error occurs during summarization.
    """
    search_summary = internet_summary_runnable.invoke(
        {"query": search_results}
    )  # Invoke the internet summary runnable with search results

    return search_summary  # Return the generated search summary


def get_chat_history() -> Optional[List[Dict[str, str]]]:
    """
    Retrieve the entire chat history from the local JSON database.

    Reads the chat history from "../../chatsDb.json" and formats it into a list of dictionaries
    suitable for conversational models, indicating 'user' or 'assistant' role for each message.

    Returns:
        Optional[List[Dict[str, str]]]: Formatted chat history as a list of dictionaries, where each
                                        dictionary has 'role' ('user' or 'assistant') and 'content'
                                        (message text). Returns None if retrieval fails.
    """
    try:
        with open("../../chatsDb.json", "r", encoding="utf-8") as f:
            db = json.load(f)  # Load chat database from JSON file

        messages = db.get("messages", [])  # Get the messages array, default to empty list if not found

        formatted_chat_history: List[Dict[str, str]] = [
            {
                "role": "user" if entry["isUser"] else "assistant",
                "content": entry["message"],
            }
            for entry in messages  # Format all messages
        ]

        return formatted_chat_history

    except Exception as e:
        print(f"Error retrieving chat history: {e}")
        return None  # Return None in case of error

### model\chat\helpers.py ###
import re
from prompts import *
import json
from html import unescape
import os
import datetime
import platform
from typing import Optional, Union, Dict, List, Any
from dotenv import load_dotenv

load_dotenv("../.env")  # Load environment variables from .env file

# --- Logging Configuration ---
# Determine log file path based on the operating system.
if platform.system() == "Windows":
    log_file_path = os.path.join(
        os.getenv("PROGRAMDATA"), "Sentient", "logs", "fastapi-backend.log"
    )
else:
    log_file_path = os.path.join("/var", "log", "sentient", "fastapi-backend.log")

# --- Utility Functions ---


def clean_key(key: str) -> str:
    """
    Clean a string by removing parentheses and extra whitespace.

    This function removes any text within parentheses (including the parentheses themselves)
    and strips leading/trailing whitespace from the given string.

    Args:
        key (str): The input string to clean.

    Returns:
        str: The cleaned string.
    """
    return re.sub(
        r"\s*\(.*?\)\s*", "", key
    ).strip()  # Remove parentheses and enclosed text, then strip whitespace


def clean_description(description: Optional[str]) -> str:
    """
    Clean a description by removing HTML tags and unescaping HTML entities.

    This function removes any HTML tags from the input description and unescapes
    HTML entities (e.g., '&' becomes '&').

    Args:
        description (Optional[str]): The description string, possibly containing HTML.

    Returns:
        str: The cleaned description string, with HTML tags removed and entities unescaped.
             Returns an empty string if the input description is None or empty.
    """
    if not description:
        return ""  # Return empty string if description is None or empty

    clean_text: str = re.sub(r"<.*?>", "", description)  # Remove HTML tags
    clean_text: str = unescape(clean_text)  # Unescape HTML entities
    return clean_text  # Return cleaned text


class TimeoutException(Exception):
    """Custom exception class for timeout events."""

    pass


def watchdog(timeout_sec: int):
    """
    Raise a TimeoutError exception with a custom message indicating a timeout.

    Args:
        timeout_sec (int): The timeout duration in seconds.

    Raises:
        TimeoutError: Always raises a TimeoutError with a message indicating the timeout duration.
    """
    raise TimeoutError(
        "Query timed out after {} seconds.".format(timeout_sec)
    )  # Raise TimeoutError with a custom message


def extract_and_fix_json(json_string: str) -> Union[Dict[str, Any], List[Any]]:
    """
    Extract and fix JSON from a possibly malformed string.

    This function attempts to extract valid JSON objects or arrays from a string that may contain
    malformed JSON. It sanitizes invalid characters, attempts to find and parse JSON matches,
    and if parsing fails, it tries to fix common syntax errors before attempting to parse again.

    Args:
        json_string (str): The input string that may contain JSON.

    Returns:
        Union[Dict[str, Any], List[Any]]: Parsed JSON object (dictionary) or array (list) if successful.
                                          Raises ValueError if JSON parsing fails even after fixing.

    Raises:
        ValueError: If no JSON object is found or if JSON parsing fails even after attempting to fix syntax.
    """
    try:
        sanitized_string: str = sanitize_invalid_characters(
            json_string
        )  # Sanitize invalid characters from JSON string

        json_matches: List[str] = re.findall(
            r"(\{.*\}|\[.*\])", sanitized_string, re.DOTALL
        )  # Find all JSON objects or arrays in the string
        if not json_matches:  # Check if any JSON matches were found
            raise ValueError(
                "No JSON object found in the input string."
            )  # Raise ValueError if no JSON object is found

        for match in json_matches:  # Iterate through each JSON match
            try:
                return json.loads(match)  # Attempt to parse the JSON match
            except json.JSONDecodeError as e:  # Catch JSONDecodeError if parsing fails
                print(f"Error in fixing JSON: {str(e)}")
                continue  # Continue to the next match if parsing fails

        fixed_json_string: str = fix_json_syntax(
            sanitized_string
        )  # Fix JSON syntax errors
        return json.loads(fixed_json_string)  # Attempt to parse the fixed JSON string

    except Exception as e:  # Catch any exceptions during JSON parsing
        print(f"Error parsing JSON: {str(e)}")
        raise ValueError(
            f"Failed to parse JSON: {e}"
        )  # Raise ValueError if JSON parsing fails


def fix_json_syntax(json_string: str) -> str:
    """
    Fix common JSON syntax issues such as missing brackets or braces.

    This function corrects common JSON syntax errors like unbalanced brackets or braces
    by counting the occurrences of opening and closing brackets and braces and appending
    or prepending the necessary characters to balance them.

    Args:
        json_string (str): The input JSON string that may have syntax errors.

    Returns:
        str: The JSON string with corrected syntax.
    """
    json_string = re.sub(
        r"^[^{\[]*", "", json_string
    )  # Remove any characters before the start of JSON object or array
    json_string = re.sub(
        r"[^}\]]*$", "", json_string
    )  # Remove any characters after the end of JSON object or array

    open_braces: int = json_string.count("{")  # Count number of opening braces
    close_braces: int = json_string.count("}")  # Count number of closing braces
    open_brackets: int = json_string.count("[")  # Count number of opening brackets
    close_brackets: int = json_string.count("]")  # Count number of closing brackets

    if (
        open_braces > close_braces
    ):  # Check if there are more opening braces than closing braces
        json_string += "}" * (
            open_braces - close_braces
        )  # Append missing closing braces
    elif (
        close_braces > open_braces
    ):  # Check if there are more closing braces than opening braces
        json_string = (
            "{" * (close_braces - open_braces) + json_string
        )  # Prepend missing opening braces

    if (
        open_brackets > close_brackets
    ):  # Check if there are more opening brackets than closing brackets
        json_string += "]" * (
            open_brackets - close_brackets
        )  # Append missing closing brackets
    elif (
        close_brackets > open_brackets
    ):  # Check if there are more closing brackets than opening brackets
        json_string = (
            "[" * (close_brackets - open_brackets) + json_string
        )  # Prepend missing opening brackets

    return json_string  # Return the JSON string with corrected syntax


def sanitize_invalid_characters(json_string: str) -> str:
    """
    Remove invalid control characters from a JSON string.

    This function removes characters that are not allowed in JSON strings,
    specifically control characters in the ranges U+0000 to U+001F and U+007F.

    Args:
        json_string (str): The input JSON string that may contain invalid characters.

    Returns:
        str: The sanitized JSON string with invalid characters removed.
    """
    invalid_chars_pattern = (
        r"[\x00-\x1F\x7F]"  # Define regex pattern for invalid control characters
    )
    return re.sub(
        invalid_chars_pattern, "", json_string
    )  # Replace invalid characters with empty string


def write_to_log(message: str):
    """
    Write a timestamped message to a log file.

    This function writes a log message to a predefined log file, prepending the current timestamp
    in ISO format. It also handles directory creation if the log directory does not exist
    and creates the log file if it does not exist.

    Args:
        message (str): The message string to write to the log file.
    """
    timestamp: str = (
        datetime.datetime.now().isoformat()
    )  # Generate ISO format timestamp
    log_message: str = f"{timestamp}: {message}\n"  # Format log message with timestamp

    try:
        os.makedirs(
            os.path.dirname(log_file_path), exist_ok=True
        )  # Ensure log directory exists, create if not

        if not os.path.exists(log_file_path):  # Check if log file exists
            with open(log_file_path, "w") as f:  # Create log file if it doesn't exist
                pass  # Do nothing, just create the file

        with open(log_file_path, "a") as log_file:  # Open log file in append mode
            log_file.write(log_message)  # Write the log message to the file
    except Exception as error:  # Catch any exceptions during log writing
        print(
            f"Error writing to log file: {error}"
        )  # Print error message if writing to log file fails

### model\chat\prompts.py ###
chat_system_prompt_template = """You are Sentient, a personalized AI companion for the user. Your primary goal is to provide responses that are engaging, empathetic, and relevant to the user's input. Follow these guidelines:

### General Rules:
1. Informal language: Keep your tone super casual and friendly for responses.
2. Contextual Personalization: If context is provided, incorporate it to generate a personalized response. DO NOT TELL THE USER ABOUT THEIR OWN PERSONALITY, SIMPLY USE IT TO GENERATE A RESPONSE.
3. Handling Empty Context: 
   - If the input is a general message and context is empty, provide a general response relevant to the input.
   - Avoid asking unnecessary follow-up questions.
4. Chat History:
   - Use the chat history to maintain continuity in conversations. 
   - If no chat history exists, respond as if it's a new conversation. DO NOT TELL THE USER THAT THERE IS NO PAST CONTEXT.
   - DO NOT REPEAT THE CHAT HISTORY. DO NOT USE WORDS LIKE "Chat History: ..." IN YOUR RESPONSE.
5. Internet Search Context:
   - If the query requires information not available in the provided context or chat history, and internet search results are provided, incorporate these results into your response.
   - Use search results to enhance the response but do not directly quote or list them unless the query explicitly asks for a detailed list.
   - Summarize the search results into coherent, user-friendly insights.

### Tone:
- For personal queries: Be empathetic, encouraging, and supportive.
- For general queries: Be concise, informative, and clear.
- Maintain a conversational and friendly tone throughout.

### Output Format:
- Responses must be relevant and directly address the query.
- Do not repeat the input unnecessarily unless for clarity.
- Seamlessly integrate internet search context when applicable.

### Examples:

#### Example 1: Personalized Response with Context
Query: "I feel overwhelmed with work."
Context: "The user is a software engineer working on a tight project deadline."
Chat History: According to the chat history, the user expressed feeling overburdened, and the assistant suggested taking breaks and focusing on manageable tasks to alleviate stress.

Response:
"It’s understandable to feel overwhelmed with such a demanding project. Maybe breaking tasks into smaller steps could help? Also, don’t hesitate to set boundaries for your work hours!"

---

#### Example 2: General Query with Empty Context
Query: "What's the weather like in Paris?"
Context: ""
Chat History: According to the chat history, the assistant mentioned they could provide updates on the weather if given the city of interest.

Response:
"I can help with that! Currently, I don't have real-time weather data, but you can check using weather apps or websites."

---

#### Example 3: Using Chat History for Continuity
Query: "Can you remind me of my goals?"
Context: "The user is working on self-improvement and wants to stay motivated."
Chat History: According to the chat history, the user mentioned focusing on building consistent habits, and the assistant suggested setting small, achievable goals.

Response:
"Of course! You mentioned focusing on consistency in your habits. Let me know if you'd like to review specific goals or create new strategies."

---

#### Example 4: Using Internet Search Context
Query: "What are the top tourist spots in Paris?"
Context: ""
Internet Search Results: "Paris, France is home to some of the world's most iconic landmarks. The Eiffel Tower offers stunning city views, the Louvre Museum houses the largest art collection, and the Notre Dame Cathedral stands as a Gothic masterpiece. Each symbolizes Paris's rich history and cultural significance, attracting millions of visitors annually."

Response:
"Paris has some amazing tourist attractions! The Eiffel Tower offers breathtaking views, while the Louvre Museum is perfect for art enthusiasts. Don’t forget to visit the Notre Dame Cathedral, a stunning example of Gothic architecture!"

---

#### Example 5: Empathetic Response
Query: "I failed my exam, and I don’t know what to do."
Context: "The user is a college student feeling stressed about academic performance."
Chat History: According to the chat history, the user expressed struggles with academic pressure, and the assistant encouraged them to focus on progress rather than perfection.

Response:
"I’m really sorry to hear that. Remember, one exam doesn’t define your abilities. Take some time to reflect and figure out what adjustments can help you moving forward. I’m here if you need advice!"

---

#### Example 6: Casual, Non-Personal Query
Query: "Tell me a fun fact."
Context: ""
Chat History: According to the chat history, the assistant shared that honey never spoils and that archaeologists found 3,000-year-old honey in ancient Egyptian tombs that was still edible.

Response:
"Here’s a fun fact: Octopuses have three hearts, and two of them stop beating when they swim!"
"""

chat_user_prompt_template = """
User Query (ANSWER THIS QUESTION OR RESPOND TO THIS MESSAGE): {query}

Context (ONLY USE THIS AS CONTEXT TO GENERATE A RESPONSE. DO NOT REPEAT THIS INFORMATION TO THE USER.): {user_context}

Internet Search Results (USE THIS AS ADDITIONAL CONTEXT TO RESPOND TO THE QUERY, ONLY IF PROVIDED.): {internet_context}

Username (ONLY CALL THE USER BY THEIR NAME WHEN REQUIRED. YOU DO NOT NEED TO CALL THE USER BY THEIR NAME IN EACH MESSAGE.): {name}

Personality (DO NOT REPEAT THE USER'S PERSONALITY TO THEM, ONLY USE IT TO GENERATE YOUR RESPONSES OR CHANGE YOUR STYLE OF TALKING.): {personality}

CHAT LIKE A HUMAN WOULD. USE LOWERCASE LETTERS AND A CASUAL TONE. AVOID USING PUNCTUATION MARKS LIKE FULL STOPS OR COMMAS. KEEP YOUR RESPONSES SHORT AND ENGAGING.
YOU NEED TO BE THE USER'S FRIEND - NOT AN ASSISTANT.
"""

### model\chat\runnables.py ###
import os
import json
import requests
from typing import Dict, Any, List, Union, Optional, Generator, Tuple
from abc import ABC, abstractmethod
from prompts import *
from dotenv import load_dotenv
import keyring
from sys import platform

load_dotenv("../.env")

def get_selected_model() -> Tuple[str, str]:
    """
    Fetches the selected model name and provider from the user profile database.

    Reads the `userProfileDb.json` file to determine the currently selected
    language model. If the database file is not found or the 'selectedModel'
    key is missing, it defaults to 'llama3.2:3b' as the model and provider.

    Returns:
        Tuple[str, str]: A tuple containing the selected model name and the provider.
                         For example: ('gpt-4o', 'openai') or ('llama3.2:3b', 'llama3.2:3b').

    Raises:
        ValueError: If the `userProfileDb.json` file path is not set or the file does not exist.
    """
    db_path = "../../userProfileDb.json"
    if not db_path or not os.path.exists(db_path):
        raise ValueError("USER_PROFILE_DB_PATH not set or file not found")
    with open(db_path, "r", encoding="utf-8") as f:
        db = json.load(f)
    selected_model = db["userData"].get("selectedModel", "llama3.2:3b")  # Default to llama3.2:3b
    if selected_model == "openai":
        return "gpt-4o", "openai"
    elif selected_model == "claude":
        return "claude-3-7-sonnet-20250219", "claude"
    else:
        return selected_model, selected_model

class BaseRunnable(ABC):
    """
    Abstract base class for runnable language model interactions.

    This class defines the interface for interacting with different language models,
    handling prompt construction, API calls, and response processing. It is designed
    to be subclassed for specific model providers like Ollama, OpenAI, Claude, and Gemini.
    """
    @abstractmethod
    def __init__(self, model_url: str, model_name: str, system_prompt_template: str,
                 user_prompt_template: str, input_variables: List[str], response_type: str,
                 required_format: Optional[Union[dict, list]] = None, stream: bool = False,
                 stateful: bool = False):
        """
        Initializes a BaseRunnable instance.

        Args:
            model_url (str): The URL of the language model API endpoint.
            model_name (str): The name or identifier of the language model.
            system_prompt_template (str): The template for the system prompt, providing context to the model.
            user_prompt_template (str): The template for the user prompt, where user inputs are inserted.
            input_variables (List[str]): A list of variable names to be replaced in the prompt templates.
            response_type (str): The expected type of the model's response ('text' or 'json').
            required_format (Optional[Union[dict, list]], optional):  Required format for JSON responses. Defaults to None.
            stream (bool, optional): Whether to enable streaming responses. Defaults to False.
            stateful (bool, optional): Whether the conversation is stateful, maintaining message history. Defaults to False.
        """
        self.model_url: str = model_url
        """The URL of the language model API endpoint."""
        self.model_name: str = model_name
        """The name or identifier of the language model."""
        self.system_prompt_template: str = system_prompt_template
        """The template for the system prompt."""
        self.user_prompt_template: str = user_prompt_template
        """The template for the user prompt."""
        self.input_variables: List[str] = input_variables
        """A list of variable names to be replaced in the prompts."""
        self.response_type: str = response_type
        """The expected type of the model's response ('text' or 'json')."""
        self.required_format: Optional[Union[dict, list]] = required_format
        """Required format for JSON responses, if applicable."""
        self.stream: bool = stream
        """Whether to use streaming for API requests."""
        self.stateful: bool = stateful
        """Whether the conversation is stateful, maintaining message history."""
        self.messages: List[Dict[str, str]] = [
            {"role": "system", "content": self.system_prompt_template}
        ]
        """Message history for stateful conversations, starting with the system prompt."""

    def build_prompt(self, inputs: Dict[str, Any]) -> None:
        """
        Builds the prompt for the language model by substituting input variables into the templates.

        Formats the user prompt template with the provided inputs and constructs
        the message history based on whether the conversation is stateful or not.
        For stateful conversations, it appends the new user prompt to the existing message history.
        For stateless conversations, it resets the message history to just the system prompt and the current user prompt.

        Args:
            inputs (Dict[str, Any]): A dictionary of input variable names and their values.
        """
        user_prompt = self.user_prompt_template.format(**inputs)

        if self.stateful:
            self.messages.append({"role": "user", "content": user_prompt})
        else:
            self.messages = [{"role": "system", "content": self.messages[0]["content"]}]
            self.messages.append({"role": "user", "content": user_prompt})

    def add_to_history(self, chat_history: List[Dict[str, str]]) -> None:
        """
        Adds chat history to the message list to maintain conversation context.

        Extends the current message list with previous conversation turns, which is
        crucial for stateful conversations where context needs to be preserved across multiple interactions.

        Args:
            chat_history (List[Dict[str, str]]): A list of message dictionaries representing the chat history.
                                                Each dictionary should have 'role' and 'content' keys.
        """
        self.messages.extend(chat_history)

    @abstractmethod
    def invoke(self, inputs: Dict[str, Any]) -> Union[Dict[str, Any], List[Any], str, None]:
        """
        Abstract method to invoke the language model with the given inputs and get a complete response.

        This method must be implemented by subclasses to handle the specific API
        call and response processing for each language model provider. It is responsible for sending
        the prompt to the model and returning the full response.

        Args:
            inputs (Dict[str, Any]): A dictionary of input variable names and their values for the prompt.

        Returns:
            Union[Dict[str, Any], List[Any], str, None]: The response from the language model.
                                                        The type of response depends on the 'response_type'
                                                        and could be a JSON object (dict), a list, a string, or None in case of error.
        """
        pass

    @abstractmethod
    def stream_response(self, inputs: Dict[str, Any]) -> Generator[Optional[str], None, None]:
        """
        Abstract method to invoke the language model and get a stream of responses.

        This method must be implemented by subclasses to handle streaming responses
        from the language model API. It should send the prompt and yield chunks of the response as they are received.

        Args:
            inputs (Dict[str, Any]): A dictionary of input variable names and their values for the prompt.

        Yields:
            Generator[Optional[str], None, None]: A generator that yields chunks of the response as strings.
                                                Yields None when the stream ends or encounters an error.
        """
        pass


class OllamaRunnable(BaseRunnable):
    """
    Runnable class for interacting with Ollama language models.

    This class extends BaseRunnable and implements the specific logic for calling
    the Ollama API, handling requests and responses, and streaming. It provides methods to invoke Ollama models
    for both complete responses and streaming responses.
    """
    def __init__(self, *args, **kwargs):
        """
        Initializes an OllamaRunnable instance.
        Inherits arguments from BaseRunnable.
        """
        super().__init__(*args, **kwargs)

    def invoke(self, inputs: Dict[str, Any]) -> Union[Dict[str, Any], str, None]:
        """
        Invokes the Ollama model to get a complete, non-streaming response.

        Constructs the payload for the Ollama API, sends the POST request, and processes
        the JSON response to extract and return the model's output.

        Args:
            inputs (Dict[str, Any]): Input variables for the prompt.

        Returns:
            Union[Dict[str, Any], str, None]: The response from the Ollama model, either JSON (dict) or text (str),
                                             or None if there is an error in the API call or response processing.
        """
        self.build_prompt(inputs)
        payload = {
            "model": self.model_name,
            "messages": self.messages,
            "stream": False,
            "options": {"num_ctx": 4096}, # Set context window size
        }

        if self.response_type == "json":  # If expecting a JSON response, set the format
            if (
                platform == "win32"
            ):  # Conditional format setting based on platform (Windows specific handling)
                payload["format"] = (
                    self.required_format
                )  # Set format directly for Windows
            else:
                payload["format"] = json.dumps(
                    self.required_format
                )  # Serialize format to JSON string for non-Windows

        response = requests.post(self.model_url, json=payload)
        return self._handle_response(response)

    def stream_response(self, inputs: Dict[str, Any]) -> Generator[Optional[str], None, None]:
        """
        Invokes the Ollama model to get a stream of responses.

        Sends a streaming POST request to the Ollama API and yields chunks of the response
        as they are received. This allows for real-time processing of the model's output.

        Args:
            inputs (Dict[str, Any]): Input variables for the prompt.

        Yields:
            Generator[Optional[str], None, None]: A generator yielding response chunks as strings.
                                                Yields None if a chunk cannot be processed or the stream ends.
        """
        self.build_prompt(inputs)
        payload = {
            "model": self.model_name,
            "messages": self.messages,
            "stream": True,
            "options": {"num_ctx": 4096}, # Set context window size
        }

        with requests.post(self.model_url, json=payload, stream=True) as response:
            for line in response.iter_lines(decode_unicode=True):
                if line:
                    yield self._handle_stream_line(line)

    def _handle_response(self, response: requests.Response) -> Union[Dict[str, Any], str, None]:
        """
        Handles the HTTP response from the Ollama API for non-streaming requests.

        Parses the JSON response, extracts the content, and handles potential errors
        such as JSON decoding failures or non-200 status codes.

        Args:
            response (requests.Response): The HTTP response object from the Ollama API.

        Returns:
            Union[Dict[str, Any], str, None]: The processed response content, either JSON (dict) or text (str).
                                             Returns None if the response status is not 200 or JSON decoding fails.

        Raises:
            ValueError: If the request fails or the JSON response cannot be decoded.
        """
        if response.status_code == 200:
            try:
                data = response.json().get("message", {}).get("content", "")
                if self.response_type == "json":
                    return json.loads(data)
                return data
            except json.JSONDecodeError as e:
                raise ValueError(f"Failed to decode JSON response: {data}. Error: {e}")
        raise ValueError(f"Request failed with status {response.status_code}: {response.text}")

    def _handle_stream_line(self, line: str) -> Optional[str]:
        """
        Handles each line of a streaming response from the Ollama API.

        Parses each line as JSON, extracts the content chunk, and returns it.
        Handles 'done' signals which indicate the end of the stream.

        Args:
            line (str): A line from the streaming response, expected to be a JSON string.

        Returns:
            Optional[str]: The extracted content chunk as a string.
                           Returns None if the line is not valid JSON, or if the stream is marked as 'done'.
        """
        try:
            data = json.loads(line)
            if data.get("done", False): # Changed from True to False, as done:True indicates stream is finished.
                return None
            return data["message"]["content"]
        except json.JSONDecodeError:
            return None


class OpenAIRunnable(BaseRunnable):
    """
    Runnable class for interacting with OpenAI language models.

    This class extends BaseRunnable and implements the specific logic for calling
    the OpenAI API, including authentication, request formatting, response handling, and streaming.
    It supports both text and JSON response formats and handles API key retrieval.
    """
    def __init__(self, *args, **kwargs):
        """
        Initializes an OpenAIRunnable instance.
        Retrieves the OpenAI API key from environment variables.
        Inherits arguments from BaseRunnable.
        """
        super().__init__(*args, **kwargs)
        self.api_key: Optional[str] = os.getenv("OPENAI_API_KEY")  # only in development
        """OpenAI API key, retrieved from environment variables."""

    def clean_schema_for_openai(self, schema: Union[Dict[str, Any], List[Any]]) -> Union[Dict[str, Any], List[Any]]:
        """
        Recursively processes the JSON schema to remove or adjust disallowed keywords like 'oneOf'
        for compatibility with OpenAI's API. OpenAI's API has limitations on certain JSON schema keywords.
        This function aims to simplify the schema to increase compatibility.

        Args:
            schema: The JSON schema to clean (can be a dict or list).

        Returns:
            The cleaned schema with unsupported fields like 'oneOf' removed or transformed.
        """
        if isinstance(schema, dict):
            if "oneOf" in schema:
                print("Warning: 'oneOf' found in schema. Replacing with first subschema.")
                # Replace 'oneOf' with the first subschema to maintain basic compatibility
                return self.clean_schema_for_openai(schema["oneOf"][0])
            # Recursively clean all other key-value pairs, excluding 'oneOf'
            return {k: self.clean_schema_for_openai(v) for k, v in schema.items() if k != "oneOf"}
        elif isinstance(schema, list):
            # Recursively clean each item in the list
            return [self.clean_schema_for_openai(item) for item in schema]
        # Return non-dict/list values unchanged (e.g., strings, numbers)
        return schema

    def invoke(self, inputs: Dict[str, Any]) -> Union[Dict[str, Any], str, None]:
        """
        Invokes the OpenAI model to get a complete, non-streaming response.

        Constructs the headers and payload for the OpenAI API, sends the POST request,
        and processes the JSON response to extract and return the model's output.
        Handles structured JSON response formatting if `response_type` is set to "json".

        Args:
            inputs: Input variables for the prompt.

        Returns:
            The response from the OpenAI model, either JSON (dict) or text (str), or None on error.

        Raises:
            ValueError: If the OpenAI API request fails or returns an error status.
        """
        # Build the prompt from inputs (assumes this sets self.messages)
        self.build_prompt(inputs)

        # Set up headers with API key
        headers = {"Authorization": f"Bearer {self.api_key}"}

        # Construct the payload
        payload = {
            "model": self.model_name,  # e.g., "gpt-4o-2024-08-06"
            "messages": self.messages,  # Contains system and user messages
            "temperature": 0.7,        # Adjustable as needed
        }

        # Apply structured JSON response format if response_type is "json"
        if self.response_type == "json":
            # Clean the schema to remove unsupported keywords like 'oneOf'
            clean_schema = self.clean_schema_for_openai(self.required_format)
            payload["response_format"] = {
                "type": "json_schema",
                "json_schema": {
                    "name": "json_response",
                    "strict": True,
                    "schema": clean_schema
                }
            }

        # Send the request to the OpenAI API
        response = requests.post(self.model_url, headers=headers, json=payload)

        # Handle and return the response
        return self._handle_response(response)

    def stream_response(self, inputs: Dict[str, Any]) -> Generator[Optional[str], None, None]:
        """
        Invokes the OpenAI model to get a stream of responses.

        Sends a streaming POST request to the OpenAI API and yields content chunks
        as they are received. This enables real-time, chunk-wise processing of the model's output.

        Args:
            inputs: Input variables for the prompt.

        Yields:
            Generator[Optional[str], None, None]: A generator yielding response chunks as strings.
                                                Yields None if a chunk is not valid or the stream ends.
        """
        self.build_prompt(inputs)
        headers = {"Authorization": f"Bearer {self.api_key}"}
        payload = {
            "model": self.model_name,
            "messages": self.messages,
            "temperature": 0.7,
            "stream": True
        }

        with requests.post(self.model_url, headers=headers, json=payload, stream=True) as response:
            for line in response.iter_lines():
                if line:
                    yield self._handle_stream_line(line)

    def _handle_response(self, response: requests.Response) -> Union[Dict[str, Any], str, None]:
        """
        Handles the HTTP response from the OpenAI API for non-streaming requests.

        Parses the JSON response, extracts the content, and handles potential errors
        such as JSON decoding failures or non-200 status codes.

        Args:
            response: The HTTP response object from the OpenAI API.

        Returns:
            The processed response content, either JSON (dict) or text (str), or None on error.

        Raises:
            ValueError: If the request to OpenAI API fails or JSON response is invalid.
        """
        if response.status_code == 200:
            data = response.json()
            content = data["choices"][0]["message"]["content"]
            if self.response_type == "json":
                try:
                    return json.loads(content)
                except json.JSONDecodeError as e:
                    raise ValueError(f"Model did not return valid JSON. Error: {e}")
            return content
        raise ValueError(f"OpenAI API Error: {response.text}")

    def _handle_stream_line(self, line: bytes) -> Optional[str]:
        """
        Handles each line of a streaming response from the OpenAI API.

        Parses each line, extracts the content delta (the incremental content chunk), and returns it.
        Streaming responses from OpenAI are in a specific 'data: ...' format, which this function parses.

        Args:
            line: A line from the streaming response in bytes.

        Returns:
            Optional[str]: The extracted content chunk as a string.
                           Returns None if the line is not a data line or if the content delta is empty.
        """
        if line.startswith(b"data: "):
            chunk = json.loads(line[6:])
            return chunk["choices"][0]["delta"].get("content", "")
        return None

class ClaudeRunnable(BaseRunnable):
    """
    Runnable class for interacting with Claude language models.

    This class extends BaseRunnable and implements the specific logic for calling
    the Claude API, including authentication, request formatting, response handling, and streaming.
    It handles API key retrieval, request headers specific to Claude, and response parsing for both
    complete and streaming responses.
    """
    def __init__(self, *args, **kwargs):
        """
        Initializes a ClaudeRunnable instance.
        Retrieves the Claude API key from environment variables.
        Inherits arguments from BaseRunnable.
        """
        super().__init__(*args, **kwargs)
        self.api_key: Optional[str] = os.getenv("CLAUDE_API_KEY") # only in development
        """Claude API key, retrieved from environment variables."""

        # # Retrieve the encrypted API key from Keyring - commented out for now.
        # encrypted_key = keyring.get_password("electron-openid-oauth", "claude")

        # # Check if the encrypted key exists
        # if encrypted_key:
        #     try:
        #         # Define the utility server URL and endpoint
        #         url = "http://localhost:5005/decrypt"
        #         # Prepare the JSON payload with the encrypted data
        #         payload = {"encrypted_data": encrypted_key}
        #         # Make a POST request to the /decrypt endpoint
        #         response = requests.post(url, json=payload)

        #         # Check if the request was successful
        #         if response.status_code == 200:
        #             # Extract the decrypted data from the response
        #             decrypted_data = response.json().get("decrypted_data")
        #             self.api_key = decrypted_data
        #         else:
        #             # Handle non-200 status codes (e.g., 500 from server errors)
        #             print(f"Failed to decrypt API key: {response.status_code} - {response.text}")
        #             self.api_key = None
        #     except requests.exceptions.RequestException as e:
        #         # Handle network-related errors (e.g., server down, connection issues)
        #         print(f"Error connecting to decryption service: {e}")
        #         self.api_key = None
        # else:
        #     # Handle the case where no encrypted key is found in Keyring
        #     print("No encrypted API key found in Keyring.")
        #     self.api_key = None

    def invoke(self, inputs: Dict[str, Any]) -> Union[Dict[str, Any], str, None]:
        """
        Invokes the Claude model to get a complete, non-streaming response.

        Constructs the headers and payload for the Claude API, sends the POST request,
        and processes the JSON response to extract and return the model's output.
        If `response_type` is "json", it adds JSON formatting instructions to the prompt to guide the model.

        Args:
            inputs (Dict[str, Any]): Input variables for the prompt.

        Returns:
            Union[Dict[str, Any], str, None]: The response from the Claude model, either JSON (dict) or text (str),
                                             or None if there is an error in the API call or response processing.

        Raises:
            ValueError: If the Claude API request fails or returns an error status.
        """
        # Build the initial prompt from inputs
        self.build_prompt(inputs)

        # If response_type is "json", modify the prompt to include formatting instructions
        if self.response_type == "json" and self.required_format:
            # Convert self.required_format to a JSON string for inclusion in the prompt
            schema_str = json.dumps(self.required_format, indent=2)
            # Add instructions to the last message (assumed to be the user message)
            instruction = (
                f"\n\nPlease format your response as a JSON object that conforms to the following schema:\n"
                f"```json\n{schema_str}\n```"
            )
            self.messages[-1]["content"] += instruction

        # Set up headers with API key and required Claude-specific headers
        headers = {
            "x-api-key": self.api_key,
            "anthropic-version": "2023-06-01", # Required Claude API version header
            "content-type": "application/json" # Explicitly set content type to JSON
        }

        # Construct the payload
        payload = {
            "model": self.model_name,  # Hardcoded model name for Claude
            "messages": self.messages,
            "max_tokens": 4096 # Maximum number of tokens in the response
        }

        # Send the request to the Claude API
        response = requests.post(self.model_url, headers=headers, json=payload)

        # Handle and return the response
        return self._handle_response(response)
    def stream_response(self, inputs: Dict[str, Any]) -> Generator[Optional[str], None, None]:
        """
        Invokes the Claude model to get a stream of responses.

        Sends a streaming POST request to the Claude API and yields content chunks
        as they are received. This allows for real-time processing of Claude's output.

        Args:
            inputs (Dict[str, Any]): Input variables for the prompt.

        Yields:
            Generator[Optional[str], None, None]: A generator yielding response chunks as strings.
                                                Yields None if a chunk is not valid or the stream ends.
        """
        self.build_prompt(inputs)
        headers = {
            "x-api-key": self.api_key,
            "anthropic-version": "2023-06-01", # Required Claude API version header
            "content-type": "application/json" # Explicitly set content type to JSON
        }
        payload = {
            "model": self.model_name,
            "messages": self.messages,
            "max_tokens": 4096, # Maximum number of tokens in the response
            "stream": True
        }

        response = requests.post(self.model_url, headers=headers, json=payload, stream=True)
        for line in response.iter_lines():
            if line:
                yield self._handle_stream_line(line)

    def _handle_response(self, response: requests.Response) -> Union[Dict[str, Any], str, None]:
        """
        Handles the HTTP response from the Claude API for non-streaming requests.

        Parses the JSON response, extracts the content, and handles potential errors
        such as JSON decoding failures or non-200 status codes.

        Args:
            response (requests.Response): The HTTP response object from the Claude API.

        Returns:
            Union[Dict[str, Any], str, None]: The processed response content, either JSON (dict) or text (str).
                                             Returns None if the response status is not 200 or JSON decoding fails.

        Raises:
            ValueError: If the request to Claude API fails or JSON response is invalid.
        """
        if response.status_code == 200:
            data = response.json()
            content = " ".join([block["text"] for block in data["content"]]) # Claude returns content as a list of blocks
            if self.response_type == "json":
                try:
                    return json.loads(content)
                except json.JSONDecodeError as e:
                    raise ValueError(f"Model did not return valid JSON. Error: {e}")
            return content
        raise ValueError(f"Claude API Error: {response.text}")

    def _handle_stream_line(self, line: bytes) -> Optional[str]:
        """
        Handles each line of a streaming response from the Claude API.

        Parses each line as JSON, extracts the content blocks, and concatenates their text.
        Claude's streaming API sends updates as JSON lines, which need to be parsed and processed.

        Args:
            line (bytes): A line from the streaming response in bytes.

        Returns:
            Optional[str]: The extracted content chunk as a string.
                           Returns None if the line is not valid JSON or if the content is empty.
        """
        try:
            data = json.loads(line.decode("utf-8"))
            return " ".join([block["text"] for block in data.get("content", [])]) # Extract text from content blocks
        except json.JSONDecodeError:
            return None

class GeminiRunnable(BaseRunnable):
    """
    Runnable class for interacting with Gemini language models.

    This class extends BaseRunnable and implements the specific logic for calling
    the Gemini API, including authentication, request formatting, response handling, and streaming.
    It manages API key authorization, request payloads formatted for Gemini, and response parsing.
    """
    def __init__(self, *args, **kwargs):
        """
        Initializes a GeminiRunnable instance.
        Retrieves the Gemini API key from environment variables.
        Inherits arguments from BaseRunnable.
        """
        super().__init__(*args, **kwargs)
        self.api_key: Optional[str] = os.getenv("GEMINI_API_KEY")  # only in development
        """Gemini API key, retrieved from environment variables."""
        # # Retrieve the encrypted API key from Keyring - commented out for now.
        # encrypted_key = keyring.get_password("electron-openid-oauth", "gemini")

        # # Check if the encrypted key exists
        # if encrypted_key:
        #     try:
        #         # Define the utility server URL and endpoint
        #         url = "http://localhost:5005/decrypt"
        #         # Prepare the JSON payload with the encrypted data
        #         payload = {"encrypted_data": encrypted_key}
        #         # Make a POST request to the /decrypt endpoint
        #         response = requests.post(url, json=payload)

        #         # Check if the request was successful
        #         if response.status_code == 200:
        #             # Extract the decrypted data from the response
        #             decrypted_data = response.json().get("decrypted_data")
        #             self.api_key = decrypted_data
        #         else:
        #             # Handle non-200 status codes (e.g., 500 from server errors)
        #             print(f"Failed to decrypt API key: {response.status_code} - {response.text}")
        #             self.api_key = None
        #     except requests.exceptions.RequestException as e:
        #         # Handle network-related errors (e.g., server down, connection issues)
        #         print(f"Error connecting to decryption service: {e}")
        #         self.api_key = None
        # else:
        #     # Handle the case where no encrypted key is found in Keyring
        #     print("No encrypted API key found in Keyring.")
        #     self.api_key = None

    def clean_schema_for_gemini(self, schema: Union[Dict[str, Any], List[Any]]) -> Union[Dict[str, Any], List[Any]]:
        """
        Cleans and adapts a JSON schema for compatibility with the Gemini API.

        Gemini has specific requirements and limitations on JSON schemas, particularly for function calling and structured output.
        This function removes unsupported keywords and adjusts the schema to align with Gemini's capabilities.
        It specifically handles 'oneOf' and ensures that object types are correctly defined.

        Args:
            schema: The JSON schema to be cleaned (can be a dict or list).

        Returns:
            The cleaned JSON schema, compatible with Gemini API requirements.
        """
        supported_keywords = {"enum", "items", "maxItems", "nullable", "properties", "required", "type"}
        if isinstance(schema, dict):
            # Handle 'oneOf' by selecting the first subschema and cleaning it
            if "oneOf" in schema:
                print("Warning: 'oneOf' found in schema. Replacing with first subschema.")
                subschema = schema["oneOf"][0]
                return self.clean_schema_for_gemini(subschema)

            cleaned = {}
            for k, v in schema.items():
                if k == "properties":
                    cleaned["properties"] = {prop: self.clean_schema_for_gemini(prop_schema)
                                            for prop, prop_schema in v.items()}
                elif k in supported_keywords:
                    cleaned[k] = self.clean_schema_for_gemini(v)

            # Ensure 'type' is set for objects with properties
            if "properties" in cleaned and "type" not in cleaned:
                cleaned["type"] = "object"

            # Handle object type validation and conversion for Gemini
            if cleaned.get("type") == "object":
                if "properties" not in cleaned or not cleaned["properties"]:
                    # Convert empty object to string type for JSON workaround
                    print("Warning: Empty object detected. Converting to JSON string schema for Gemini compatibility.")
                    return {
                        "type": "string",
                        "description": cleaned.get("description", "Dynamic parameters as a JSON string") + " (JSON string)"
                    }
                if "required" in cleaned:
                    defined_properties = set(cleaned["properties"].keys())
                    cleaned["required"] = [prop for prop in cleaned["required"] if prop in defined_properties]
                    if not cleaned["required"]:
                        del cleaned["required"]

            return cleaned
        elif isinstance(schema, list):
            return [self.clean_schema_for_gemini(item) for item in schema]
        return schema

    def invoke(self, inputs: Dict[str, Any]) -> Union[Dict[str, Any], str, None]:
        """
        Invokes the Gemini model to get a complete, non-streaming response.

        Constructs the payload for the Gemini API, sends the POST request, and processes
        the JSON response to extract and return the model's output.
        Handles system instructions and formats the payload according to Gemini's API requirements.

        Args:
            inputs (Dict[str, Any]): Input variables for the prompt.

        Returns:
            Union[Dict[str, Any], str, None]: The response from the Gemini model, either JSON (dict) or text (str),
                                             or None if there is an error in the API call or response processing.

        Raises:
            ValueError: If the Gemini API request fails or returns an error status.
        """
        self.build_prompt(inputs)
        system_instruction = None
        if self.messages and self.messages[0]["role"].lower() == "system":
            system_content = self.messages[0]["content"]
            system_instruction = {"parts": [{"text": system_content}]}
            conversation_messages = self.messages[1:]
        else:
            conversation_messages = self.messages

        def map_role(role: str) -> str:
            """Maps a role name to Gemini's role names ('user' or 'model')."""
            return "model" if role.lower() == "assistant" else "user"

        contents = [{"role": map_role(msg["role"]), "parts": [{"text": msg["content"]}]}
                    for msg in conversation_messages]
        payload = {"contents": contents}
        if system_instruction:
            payload["systemInstruction"] = system_instruction

        if self.response_type == "json" and self.required_format:
            generation_config = {"response_mime_type": "application/json"}
            if self.required_format is not None:
                clean_schema = self.clean_schema_for_gemini(self.required_format)
                generation_config["response_schema"] = clean_schema
            payload["generationConfig"] = generation_config

        response = requests.post(
            f"{self.model_url}?key={self.api_key}", # API key is passed as a query parameter
            json=payload,
            headers={"Content-Type": "application/json"} # Explicitly set content type to JSON
        )
        return self._handle_response(response)

    def _handle_response(self, response: requests.Response) -> Union[Dict[str, Any], str, None]:
        """
        Handles the HTTP response from the Gemini API for non-streaming requests.

        Parses the JSON response, extracts the content, and handles potential errors
        such as JSON decoding failures or non-200 status codes.

        Args:
            response (requests.Response): The HTTP response object from the Gemini API.

        Returns:
            Union[Dict[str, Any], str, None]: The processed response content, either JSON (dict) or text (str).
                                             Returns None if the response status is not 200 or JSON decoding fails.

        Raises:
            ValueError: If the request to Gemini API fails or JSON response is invalid.
        """
        if response.status_code == 200:
            data = response.json()
            content = "".join([part["text"] for part in data["candidates"][0]["content"]["parts"]]) # Gemini returns content in 'parts'
            if self.response_type == "json" and self.required_format:
                try:
                    return json.loads(content)
                except json.JSONDecodeError as e:
                    raise ValueError(f"Model did not return valid JSON. Error: {e}")
            return content
        raise ValueError(f"Gemini API Error: {response.text}")

    def stream_response(self, inputs: Dict[str, Any]) -> Generator[Union[Dict[str, Any], str, None], None, None]:
        """
        Invokes the Gemini model to get a stream of responses.

        Currently implemented as a single yield due to limited streaming support,
        but can be extended for true streaming later. In the current implementation, it behaves like a non-streaming call.

        Args:
            inputs (Dict[str, Any]): Input variables for the prompt.

        Yields:
            Generator[Union[Dict[str, Any], str, None], None, None]: A generator yielding the response.
                                                                    In the current implementation, it yields only once.
        """
        self.build_prompt(inputs)

        system_instruction = None
        if self.messages and self.messages[0]["role"].lower() == "system":
            system_content = self.messages[0]["content"]
            system_instruction = {"parts": [{"text": system_content}]}
            conversation_messages = self.messages[1:]
        else:
            conversation_messages = self.messages

        def map_role(role: str) -> str:
            """Maps a role name to Gemini's role names ('user' or 'model')."""
            role_lower = role.lower()
            return "model" if role_lower == "assistant" else "user"

        contents = [{"role": map_role(msg["role"]), "parts": [{"text": msg["content"]}]}
                    for msg in conversation_messages]

        payload: Dict[str, Any] = {"contents": contents}
        if system_instruction:
            payload["systemInstruction"] = system_instruction

        response = requests.post(
            f"{self.model_url}?key={self.api_key}", # API key is passed as a query parameter
            json=payload,
            headers={"Content-Type": "application/json"} # Explicitly set content type to JSON
        )

        yield self._handle_response(response)

def get_chat_runnable(chat_history: List[Dict[str, str]]) -> BaseRunnable:
    """
    Factory function to get the appropriate Runnable class based on the selected model.

    Determines the model provider (Ollama, OpenAI, Claude, Gemini) based on the
    selected model name from `get_selected_model()`. It then returns an instance
    of the corresponding Runnable class, configured with API URLs and default prompts.

    Args:
        chat_history (List[Dict[str, str]]): The chat history to initialize the runnable with.

    Returns:
        BaseRunnable: An instance of a Runnable class (OllamaRunnable, OpenAIRunnable, ClaudeRunnable, or GeminiRunnable).
    """
    model_mapping: Dict[str, tuple[Optional[str], type[BaseRunnable]]] = {
        "openai": (os.getenv("OPENAI_API_URL"), OpenAIRunnable),
        "claude": (os.getenv("CLAUDE_API_URL"), ClaudeRunnable),
        "gemini": (os.getenv("GEMINI_API_URL"), GeminiRunnable),
    }

    model_name, provider = get_selected_model()

     # Extract provider from model name (e.g., 'openai' from 'openai:gpt-3.5-turbo')


    if provider and provider in model_mapping:
        model_url, runnable_class = model_mapping[provider]
    else:
        model_url = os.getenv("BASE_MODEL_URL")
        runnable_class = OllamaRunnable

    runnable: BaseRunnable = runnable_class(
        model_url=model_url,
        model_name=model_name,
        system_prompt_template=chat_system_prompt_template,
        user_prompt_template=chat_user_prompt_template,
        input_variables=[
            "query",
            "user_context",
            "internet_context",
            "name",
            "personality",
        ],
        response_type="chat",
        stream=True,
        stateful=True,
    )

    runnable.add_to_history(chat_history)
    return runnable
### model\chat\startserv.sh ###
sudo -E <your-venv-path> -m uvicorn chat:app --host 0.0.0.0 --port 5003
